{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0978047-f83f-4cb8-9684-b9a18f078c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "\n",
    "import copy\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "767a2537-a194-481e-a44e-31e2be65dea4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_original_dataset, load_deleted_dataset\n",
    "from models import CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99c348f3-d7b0-4c5b-981d-f564e7634174",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = 'Datasets/Features/'\n",
    "EPOCHS = 5\n",
    "PERCENTAGES = [1, 10, 20, 30, 40, 50, 60, 70, 80, 90, 99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e04b45f-bfab-408f-beb8-1cf3200194b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For MNIST dataset: python3 main.py --bz 16384 --epochs 20 --model Logistic_regression --dataset MNIST --wd 0.0001 --lr 0.1 0.05 --lrlen 10 10 --method deltagrad --period 5 --init 20 -m 2 --cached_size 20\n",
    "is_GPU = True\n",
    "device = 0\n",
    "BATCH_SIZE = 1024\n",
    "LR = 0.05\n",
    "WD = 0.0001\n",
    "INIT_EPOCHS = 1\n",
    "PERIOD = 2\n",
    "M = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "644d9979-8201-4ac9-acfe-d2a9f28a9608",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath('./libraries/DeltaGrad/src/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ebd8b61-fb04-4ad1-9c58-b7be0c2be64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import post_processing_gradien_para_list_all_epochs, append_gradient_list, init_model, get_model_para_shape_list, get_devectorized_parameters, get_all_vectorized_parameters1, compute_model_para_diff, compute_derivative_one_more_step\n",
    "from main_delete import explicit_iters, compute_grad_final3, cal_approx_hessian_vec_prod0_3, compute_approx_hessian_vector_prod_with_prepared_terms1, prepare_hessian_vec_prod0_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3cb844c-6f77-42d3-86cf-b1e8996b83dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/thuwuyinjun/DeltaGrad/blob/master/src/main_delete.py#L362\n",
    "def update_para_final2(para, gradient_list, alpha):\n",
    "    \n",
    "    vec_para = get_all_vectorized_parameters1(para)\n",
    "    \n",
    "    vec_para -= alpha*gradient_list\n",
    "        \n",
    "    return vec_para\n",
    "\n",
    "# https://github.com/thuwuyinjun/DeltaGrad/blob/master/src/Models/DNN_single.py#L105\n",
    "class DGCNN(CNN):\n",
    "    def get_all_gradient(self):\n",
    "        \n",
    "        para_list = []\n",
    "        \n",
    "        for param in self.parameters():\n",
    "            para_list.append(param.grad.clone())\n",
    "            \n",
    "        return para_list    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9a8c382-73d8-433e-97df-dae28c3dc6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/thuwuyinjun/DeltaGrad/blob/master/src/main_delete.py#L1176"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a10809aa-28f8-4231-9515-301235d897ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/thuwuyinjun/DeltaGrad/blob/master/README.md?plain=1#L77\n",
    "# update the model after the training phase with deltagrad\n",
    "\n",
    "# https://github.com/thuwuyinjun/DeltaGrad/blob/master/src/main.py#L107\n",
    "\n",
    "def fit(model, save_dir, train_set, test_set, forget_set):\n",
    "    \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # https://github.com/thuwuyinjun/DeltaGrad/blob/master/src/Models/Data_preparer.py#L325\n",
    "    # replace softmax+nlloss with cross_entropy\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    # prepare model\n",
    "    # https://github.com/thuwuyinjun/DeltaGrad/blob/master/src/main_delete.py#L1133\n",
    "    net = copy.deepcopy(model)\n",
    "    \n",
    "    # https://github.com/thuwuyinjun/DeltaGrad/blob/master/src/Models/Data_preparer.py#L326\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=LR, weight_decay=WD)\n",
    "    net_optimizer = torch.optim.SGD(net.parameters(), lr=LR, weight_decay=WD)\n",
    "\n",
    "    train_batch_size = int(np.ceil(BATCH_SIZE * len(train_set) / (len(train_set) + len(forget_set))))\n",
    "    forget_batch_size = int(np.ceil(BATCH_SIZE * len(forget_set) / (len(train_set) + len(forget_set))))\n",
    "\n",
    "    num_steps = min(len(train_set) // train_batch_size, len(forget_set) // forget_batch_size)\n",
    "\n",
    "    train_x, train_y = train_set.tensors[0], train_set.tensors[1]\n",
    "    forget_x, forget_y = forget_set.tensors[0], forget_set.tensors[1]\n",
    "\n",
    "    # https://github.com/thuwuyinjun/DeltaGrad/blob/master/src/main_delete.py#L1202\n",
    "    # https://github.com/thuwuyinjun/DeltaGrad/blob/master/src/main_delete.py#L815\n",
    "\n",
    "    # https://github.com/thuwuyinjun/DeltaGrad/blob/master/src/main_delete.py#L819\n",
    "    para = list(model.parameters())\n",
    "    # https://github.com/thuwuyinjun/DeltaGrad/blob/master/src/main_delete.py#L826\n",
    "    full_shape_list, shape_list, total_shape_size = get_model_para_shape_list(model.parameters())\n",
    "\n",
    "    # https://github.com/thuwuyinjun/DeltaGrad/blob/master/src/main_delete.py#L836\n",
    "    S_k_list = deque()\n",
    "    Y_k_list = deque()\n",
    "    \n",
    "    train_times = list()\n",
    "    \n",
    "    train_accs, test_accs, forget_accs = list(), list(), list()\n",
    "    \n",
    "    for epoch in range(EPOCHS):    \n",
    "        \n",
    "        # train\n",
    "        \n",
    "        train_time = 0\n",
    "        \n",
    "        start_time = time.time()\n",
    "\n",
    "        model.train()\n",
    "        for i in range(num_steps):\n",
    "\n",
    "            # https://github.com/thuwuyinjun/DeltaGrad/blob/master/src/main_delete.py#L890\n",
    "\n",
    "            batch_remaining_X = train_x[train_batch_size*i:train_batch_size*(i+1)].cuda()\n",
    "            batch_remaining_Y = train_y[train_batch_size*i:train_batch_size*(i+1)].cuda()\n",
    "\n",
    "            # https://github.com/thuwuyinjun/DeltaGrad/blob/master/src/main_delete.py#L930\n",
    "\n",
    "            batch_delta_X = forget_x[forget_batch_size*i:forget_batch_size*(i+1)].cuda()\n",
    "            batch_delta_Y = forget_y[forget_batch_size*i:forget_batch_size*(i+1)].cuda()\n",
    "\n",
    "            # https://github.com/thuwuyinjun/DeltaGrad/blob/master/src/main_delete.py#L895\n",
    "\n",
    "            curr_matched_ids_size = batch_delta_X.shape[0]\n",
    "\n",
    "            # https://github.com/thuwuyinjun/DeltaGrad/blob/master/src/model_train.py#L18\n",
    "            \n",
    "            net_optimizer.zero_grad()\n",
    "            output = net(torch.concat([batch_remaining_X, batch_delta_X], dim=0))\n",
    "            loss = criterion(output, torch.concat([batch_remaining_Y, batch_delta_Y], dim=0))\n",
    "            loss.backward()\n",
    "            net_optimizer.step()\n",
    "\n",
    "            gradient_list = []\n",
    "            para_list = []\n",
    "            append_gradient_list(gradient_list, None, para_list, net, None, is_GPU, device)\n",
    "\n",
    "            para_list_tensor, grad_list_tensor = post_processing_gradien_para_list_all_epochs(para_list, gradient_list)\n",
    "            para_list_tensor, grad_list_tensor = para_list_tensor.cuda(), grad_list_tensor.cuda()\n",
    "                \n",
    "            # https://github.com/thuwuyinjun/DeltaGrad/blob/master/src/main_delete.py#L927\n",
    "\n",
    "            if epoch < INIT_EPOCHS:\n",
    "\n",
    "                para, _, init_hessian_para_prod, theta_k = explicit_iters(\n",
    "                    batch_delta_X, batch_delta_Y, batch_remaining_X, batch_remaining_Y, \n",
    "                    curr_matched_ids_size, model, para, epoch, i, M+1, S_k_list, Y_k_list, LR, WD, \n",
    "                    grad_list_tensor, grad_list_tensor, 0, full_shape_list, shape_list, \n",
    "                    is_GPU, device, \n",
    "                    criterion, optimizer, None, None\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                \n",
    "                '''use l-bfgs algorithm to evaluate the gradients'''\n",
    "\n",
    "                # https://github.com/thuwuyinjun/DeltaGrad/blob/master/src/main_delete.py#L952\n",
    "                \n",
    "                init_model(model, para)\n",
    "\n",
    "                compute_derivative_one_more_step(model, batch_delta_X, batch_delta_Y, criterion, optimizer)\n",
    "                \n",
    "                gradient_dual = model.get_all_gradient()\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                \n",
    "                    vec_para_diff = torch.t((get_all_vectorized_parameters1(para) - para_list_tensor))\n",
    "                    \n",
    "                    if (epoch - INIT_EPOCHS) / PERIOD >= 1:\n",
    "                        if (epoch - INIT_EPOCHS) % PERIOD == 0:\n",
    "                            zero_mat_dim, curr_Y_k, curr_S_k, sigma_k, mat_prime = prepare_hessian_vec_prod0_3(list(S_k_list)[1:], list(Y_k_list)[1:], i, INIT_EPOCHS, M, is_GPU, device)\n",
    "                            \n",
    "                            mat = np.linalg.inv(mat_prime.cpu().numpy())\n",
    "                            mat = torch.from_numpy(mat)\n",
    "                            mat = mat.to(device)\n",
    "                            \n",
    "                        hessian_para_prod = compute_approx_hessian_vector_prod_with_prepared_terms1(zero_mat_dim, curr_Y_k, curr_S_k, sigma_k, mat, vec_para_diff, is_GPU, device)\n",
    "                        \n",
    "                    else:\n",
    "                        '''S_k_list, Y_k_list, v_vec, k, is_GPU, device'''\n",
    "                        hessian_para_prod, zero_mat_dim, curr_Y_k, curr_S_k, sigma_k, mat_prime = cal_approx_hessian_vec_prod0_3(list(S_k_list)[1:], list(Y_k_list)[1:], vec_para_diff, M, is_GPU, device)\n",
    "                    \n",
    "                    is_positive, final_gradient_list = compute_grad_final3(\n",
    "                        get_all_vectorized_parameters1(para), torch.t(hessian_para_prod), \n",
    "                        get_all_vectorized_parameters1(gradient_dual), \n",
    "                        grad_list_tensor, para_list_tensor, \n",
    "                        batch_remaining_X.shape[0] + curr_matched_ids_size, curr_matched_ids_size, \n",
    "                        LR, WD, is_GPU, device\n",
    "                    )\n",
    "                        \n",
    "                    vec_para = update_para_final2(para, final_gradient_list, LR)\n",
    "                    \n",
    "                    para = get_devectorized_parameters(vec_para, full_shape_list, shape_list)\n",
    "                \n",
    "\n",
    "        \n",
    "        train_time += time.time() - start_time\n",
    "            \n",
    "        train_times.append(train_time)\n",
    "        \n",
    "        # test\n",
    "            \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            #\n",
    "            \n",
    "            x, y = train_set.tensors\n",
    "            \n",
    "            accs = list()\n",
    "            \n",
    "            for i in range(0, x.shape[0], BATCH_SIZE):\n",
    "            \n",
    "                output = model(x[i:i+BATCH_SIZE].cuda())\n",
    "\n",
    "                predicted = torch.argmax(output.data, dim=-1)\n",
    "                accs.append((predicted == y[i:i+BATCH_SIZE].cuda()).float().mean().detach().cpu().numpy())\n",
    "            \n",
    "            train_accs.append(np.mean(accs))\n",
    "            \n",
    "            #\n",
    "            \n",
    "            x, y = test_set.tensors\n",
    "            \n",
    "            accs = list()\n",
    "            \n",
    "            for i in range(0, x.shape[0], BATCH_SIZE):\n",
    "            \n",
    "                output = model(x[i:i+BATCH_SIZE].cuda())\n",
    "\n",
    "                predicted = torch.argmax(output.data, dim=-1)\n",
    "                accs.append((predicted == y[i:i+BATCH_SIZE].cuda()).float().mean().detach().cpu().numpy())\n",
    "            \n",
    "            test_accs.append(np.mean(accs))\n",
    "            \n",
    "            #\n",
    "\n",
    "            x, y = forget_set.tensors\n",
    "            \n",
    "            accs = list()\n",
    "\n",
    "            for i in range(0, x.shape[0], BATCH_SIZE):\n",
    "\n",
    "                output = model(x[i:i+BATCH_SIZE].cuda())\n",
    "\n",
    "                predicted = torch.argmax(output.data, dim=-1)\n",
    "                accs.append((predicted == y[i:i+BATCH_SIZE].cuda()).float().mean().detach().cpu().numpy())\n",
    "\n",
    "            forget_accs.append(np.mean(accs))\n",
    "        \n",
    "        # save\n",
    "        torch.save(model.state_dict(), os.path.join(save_dir, f'{(epoch+1):03d}.pt'))\n",
    "\n",
    "    return train_times, train_accs, test_accs, forget_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "147b38b7-2f8b-44e9-88cd-572bd54ee2f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ace10aacef54bf99b10317908537caa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = list()\n",
    "\n",
    "for percentage in tqdm(PERCENTAGES):\n",
    "\n",
    "    # https://github.com/thuwuyinjun/DeltaGrad/blob/master/src/main_delete.py#L1124\n",
    "    # https://github.com/thuwuyinjun/DeltaGrad/blob/master/src/Models/DNN_single.py#L47\n",
    "    # remove softmax\n",
    "    model = DGCNN().cuda()\n",
    "        \n",
    "    # https://github.com/thuwuyinjun/DeltaGrad/blob/master/src/main_delete.py#L1133\n",
    "    model.load_state_dict(torch.load('./weights/init.pt'))\n",
    "    \n",
    "    train_set, test_set, forget_set = load_deleted_dataset(DATA_DIR, percentage)\n",
    "    \n",
    "    train_times, train_accs, test_accs, forget_accs = fit(model, f'weights/DeltaGrad/{percentage}', train_set, test_set, forget_set)\n",
    "    \n",
    "    df = pd.DataFrame(zip(train_times, train_accs, test_accs, forget_accs), columns=['train_time', 'train_acc', 'test_acc', 'forget_acc'])\n",
    "    df['epoch'] = range(1, EPOCHS+1)\n",
    "    df['percentage'] = percentage\n",
    "    \n",
    "    results.append(df)\n",
    "\n",
    "results = pd.concat(results).set_index(['percentage', 'epoch'])\n",
    "\n",
    "results.to_csv(f'results/DeltaGrad.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b6eda0e-5b3c-432a-8be8-7e85dfbbf08a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>train_time</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>test_acc</th>\n",
       "      <th>forget_acc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>percentage</th>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th>1</th>\n",
       "      <td>2.798030</td>\n",
       "      <td>0.684852</td>\n",
       "      <td>0.692215</td>\n",
       "      <td>0.691667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.019802</td>\n",
       "      <td>0.097077</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.096667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.956517</td>\n",
       "      <td>0.097077</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.096667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.979096</td>\n",
       "      <td>0.097077</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.096667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.321297</td>\n",
       "      <td>0.097077</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.096667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">10</th>\n",
       "      <th>1</th>\n",
       "      <td>2.231315</td>\n",
       "      <td>0.222672</td>\n",
       "      <td>0.220566</td>\n",
       "      <td>0.224216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.166035</td>\n",
       "      <td>0.098714</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.098991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.136526</td>\n",
       "      <td>0.098714</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.098991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.113361</td>\n",
       "      <td>0.098714</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.098991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.438504</td>\n",
       "      <td>0.098714</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.098991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">20</th>\n",
       "      <th>1</th>\n",
       "      <td>2.225544</td>\n",
       "      <td>0.585216</td>\n",
       "      <td>0.586904</td>\n",
       "      <td>0.583623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.249819</td>\n",
       "      <td>0.098268</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.100724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.200084</td>\n",
       "      <td>0.098268</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.100724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.176492</td>\n",
       "      <td>0.098268</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.100724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.501146</td>\n",
       "      <td>0.098268</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.100724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">30</th>\n",
       "      <th>1</th>\n",
       "      <td>2.222276</td>\n",
       "      <td>0.794317</td>\n",
       "      <td>0.809740</td>\n",
       "      <td>0.786604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.346921</td>\n",
       "      <td>0.097726</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.098793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.300222</td>\n",
       "      <td>0.097726</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.098793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.300544</td>\n",
       "      <td>0.097726</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.098793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.557895</td>\n",
       "      <td>0.097726</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.098793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">40</th>\n",
       "      <th>1</th>\n",
       "      <td>2.249781</td>\n",
       "      <td>0.105284</td>\n",
       "      <td>0.106583</td>\n",
       "      <td>0.105236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.413353</td>\n",
       "      <td>0.098573</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.099080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.431201</td>\n",
       "      <td>0.098573</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.099080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.385542</td>\n",
       "      <td>0.098573</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.099080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.656636</td>\n",
       "      <td>0.098573</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.099080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">50</th>\n",
       "      <th>1</th>\n",
       "      <td>2.193686</td>\n",
       "      <td>0.098578</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.098468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.475509</td>\n",
       "      <td>0.098578</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.098468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.451005</td>\n",
       "      <td>0.098578</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.098468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.387493</td>\n",
       "      <td>0.098578</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.098468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.726167</td>\n",
       "      <td>0.098578</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.098468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">60</th>\n",
       "      <th>1</th>\n",
       "      <td>2.216934</td>\n",
       "      <td>0.354998</td>\n",
       "      <td>0.351469</td>\n",
       "      <td>0.354704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.569359</td>\n",
       "      <td>0.099406</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.098210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.568106</td>\n",
       "      <td>0.099406</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.098210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.573989</td>\n",
       "      <td>0.099406</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.098210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.815980</td>\n",
       "      <td>0.099406</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.098210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">70</th>\n",
       "      <th>1</th>\n",
       "      <td>2.203233</td>\n",
       "      <td>0.310677</td>\n",
       "      <td>0.307605</td>\n",
       "      <td>0.306548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.710657</td>\n",
       "      <td>0.098867</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.100725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.771116</td>\n",
       "      <td>0.098867</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.100725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.674032</td>\n",
       "      <td>0.098867</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.100725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.052062</td>\n",
       "      <td>0.098867</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.100725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">80</th>\n",
       "      <th>1</th>\n",
       "      <td>2.236583</td>\n",
       "      <td>0.099429</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.098532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.763274</td>\n",
       "      <td>0.099429</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.098532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.816030</td>\n",
       "      <td>0.099429</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.098532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.756171</td>\n",
       "      <td>0.099429</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.098532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.109777</td>\n",
       "      <td>0.099429</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.098532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">90</th>\n",
       "      <th>1</th>\n",
       "      <td>2.259893</td>\n",
       "      <td>0.097831</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.098866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.903175</td>\n",
       "      <td>0.097831</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.098866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.863327</td>\n",
       "      <td>0.097831</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.098866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.812195</td>\n",
       "      <td>0.097831</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.098866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.139593</td>\n",
       "      <td>0.097831</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.098866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">99</th>\n",
       "      <th>1</th>\n",
       "      <td>2.111590</td>\n",
       "      <td>0.096667</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.097077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.681713</td>\n",
       "      <td>0.096667</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.097077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.716931</td>\n",
       "      <td>0.096667</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.097077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.699249</td>\n",
       "      <td>0.096667</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.097077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.062720</td>\n",
       "      <td>0.096667</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.097077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  train_time  train_acc  test_acc  forget_acc\n",
       "percentage epoch                                             \n",
       "1          1        2.798030   0.684852  0.692215    0.691667\n",
       "           2        2.019802   0.097077  0.098184    0.096667\n",
       "           3        1.956517   0.097077  0.098184    0.096667\n",
       "           4        1.979096   0.097077  0.098184    0.096667\n",
       "           5        1.321297   0.097077  0.098184    0.096667\n",
       "10         1        2.231315   0.222672  0.220566    0.224216\n",
       "           2        2.166035   0.098714  0.098184    0.098991\n",
       "           3        2.136526   0.098714  0.098184    0.098991\n",
       "           4        2.113361   0.098714  0.098184    0.098991\n",
       "           5        1.438504   0.098714  0.098184    0.098991\n",
       "20         1        2.225544   0.585216  0.586904    0.583623\n",
       "           2        2.249819   0.098268  0.098184    0.100724\n",
       "           3        2.200084   0.098268  0.098184    0.100724\n",
       "           4        2.176492   0.098268  0.098184    0.100724\n",
       "           5        1.501146   0.098268  0.098184    0.100724\n",
       "30         1        2.222276   0.794317  0.809740    0.786604\n",
       "           2        2.346921   0.097726  0.098184    0.098793\n",
       "           3        2.300222   0.097726  0.098184    0.098793\n",
       "           4        2.300544   0.097726  0.098184    0.098793\n",
       "           5        1.557895   0.097726  0.098184    0.098793\n",
       "40         1        2.249781   0.105284  0.106583    0.105236\n",
       "           2        2.413353   0.098573  0.098184    0.099080\n",
       "           3        2.431201   0.098573  0.098184    0.099080\n",
       "           4        2.385542   0.098573  0.098184    0.099080\n",
       "           5        1.656636   0.098573  0.098184    0.099080\n",
       "50         1        2.193686   0.098578  0.098184    0.098468\n",
       "           2        2.475509   0.098578  0.098184    0.098468\n",
       "           3        2.451005   0.098578  0.098184    0.098468\n",
       "           4        2.387493   0.098578  0.098184    0.098468\n",
       "           5        1.726167   0.098578  0.098184    0.098468\n",
       "60         1        2.216934   0.354998  0.351469    0.354704\n",
       "           2        2.569359   0.099406  0.098184    0.098210\n",
       "           3        2.568106   0.099406  0.098184    0.098210\n",
       "           4        2.573989   0.099406  0.098184    0.098210\n",
       "           5        1.815980   0.099406  0.098184    0.098210\n",
       "70         1        2.203233   0.310677  0.307605    0.306548\n",
       "           2        2.710657   0.098867  0.098184    0.100725\n",
       "           3        2.771116   0.098867  0.098184    0.100725\n",
       "           4        2.674032   0.098867  0.098184    0.100725\n",
       "           5        2.052062   0.098867  0.098184    0.100725\n",
       "80         1        2.236583   0.099429  0.098184    0.098532\n",
       "           2        2.763274   0.099429  0.098184    0.098532\n",
       "           3        2.816030   0.099429  0.098184    0.098532\n",
       "           4        2.756171   0.099429  0.098184    0.098532\n",
       "           5        2.109777   0.099429  0.098184    0.098532\n",
       "90         1        2.259893   0.097831  0.098184    0.098866\n",
       "           2        2.903175   0.097831  0.098184    0.098866\n",
       "           3        2.863327   0.097831  0.098184    0.098866\n",
       "           4        2.812195   0.097831  0.098184    0.098866\n",
       "           5        2.139593   0.097831  0.098184    0.098866\n",
       "99         1        2.111590   0.096667  0.098184    0.097077\n",
       "           2        2.681713   0.096667  0.098184    0.097077\n",
       "           3        2.716931   0.096667  0.098184    0.097077\n",
       "           4        2.699249   0.096667  0.098184    0.097077\n",
       "           5        2.062720   0.096667  0.098184    0.097077"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542d9cae-795a-4bc7-8e05-883e2315f4e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
