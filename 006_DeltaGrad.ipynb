{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b0978047-f83f-4cb8-9684-b9a18f078c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "\n",
    "import copy\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "767a2537-a194-481e-a44e-31e2be65dea4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_original_dataset, load_deleted_dataset\n",
    "from models import CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99c348f3-d7b0-4c5b-981d-f564e7634174",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = 'Datasets/Features/'\n",
    "EPOCHS = 5\n",
    "PERCENTAGES = [1, 10, 20, 30, 40, 50, 60, 70, 80, 90, 99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "8e04b45f-bfab-408f-beb8-1cf3200194b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For MNIST dataset: python3 main.py --bz 16384 --epochs 20 --model Logistic_regression --dataset MNIST --wd 0.0001 --lr 0.1 0.05 --lrlen 10 10 --method deltagrad --period 5 --init 20 -m 2 --cached_size 20\n",
    "is_GPU = True\n",
    "device = 0\n",
    "BATCH_SIZE = 1024\n",
    "LR = 0.05\n",
    "WD = 0.0001\n",
    "INIT_EPOCHS = 1\n",
    "PERIOD = 2\n",
    "M = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5881e5a-ecba-4bc9-b1a2-2ea588462662",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs('results/DeltaGrad', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "644d9979-8201-4ac9-acfe-d2a9f28a9608",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath('./libraries/DeltaGrad/src/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5ebd8b61-fb04-4ad1-9c58-b7be0c2be64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import post_processing_gradien_para_list_all_epochs, append_gradient_list, init_model, get_model_para_shape_list, get_devectorized_parameters, get_all_vectorized_parameters1, compute_model_para_diff, compute_derivative_one_more_step\n",
    "from main_delete import explicit_iters, compute_grad_final3, cal_approx_hessian_vec_prod0_3, compute_approx_hessian_vector_prod_with_prepared_terms1, prepare_hessian_vec_prod0_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d3cb844c-6f77-42d3-86cf-b1e8996b83dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/thuwuyinjun/DeltaGrad/blob/master/src/main_delete.py#L362\n",
    "def update_para_final2(para, gradient_list, alpha):\n",
    "    \n",
    "    vec_para = get_all_vectorized_parameters1(para)\n",
    "    \n",
    "    vec_para -= alpha*gradient_list\n",
    "        \n",
    "    return vec_para\n",
    "\n",
    "# https://github.com/thuwuyinjun/DeltaGrad/blob/master/src/Models/DNN_single.py#L105\n",
    "class DGCNN(CNN):\n",
    "    def get_all_gradient(self):\n",
    "        \n",
    "        para_list = []\n",
    "        \n",
    "        for param in self.parameters():\n",
    "            para_list.append(param.grad.clone())\n",
    "            \n",
    "        return para_list    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9a8c382-73d8-433e-97df-dae28c3dc6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/thuwuyinjun/DeltaGrad/blob/master/src/main_delete.py#L1176"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a10809aa-28f8-4231-9515-301235d897ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/thuwuyinjun/DeltaGrad/blob/master/README.md?plain=1#L77\n",
    "# update the model after the training phase with deltagrad\n",
    "\n",
    "# https://github.com/thuwuyinjun/DeltaGrad/blob/master/src/main.py#L107\n",
    "\n",
    "def fit(model, save_dir, train_set, test_set, forget_set):\n",
    "    \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # https://github.com/thuwuyinjun/DeltaGrad/blob/master/src/Models/Data_preparer.py#L325\n",
    "    # replace softmax+nlloss with cross_entropy\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    # prepare model\n",
    "    # https://github.com/thuwuyinjun/DeltaGrad/blob/master/src/main_delete.py#L1133\n",
    "    net = copy.deepcopy(model)\n",
    "    \n",
    "    # https://github.com/thuwuyinjun/DeltaGrad/blob/master/src/Models/Data_preparer.py#L326\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=LR, weight_decay=WD)\n",
    "    net_optimizer = torch.optim.SGD(net.parameters(), lr=LR, weight_decay=WD)\n",
    "\n",
    "    train_batch_size = int(np.ceil(BATCH_SIZE * len(train_set) / (len(train_set) + len(forget_set))))\n",
    "    forget_batch_size = int(np.ceil(BATCH_SIZE * len(forget_set) / (len(train_set) + len(forget_set))))\n",
    "\n",
    "    num_steps = min(len(train_set) // train_batch_size, len(forget_set) // forget_batch_size)\n",
    "\n",
    "    train_x, train_y = train_set.tensors[0], train_set.tensors[1]\n",
    "    forget_x, forget_y = forget_set.tensors[0], forget_set.tensors[1]\n",
    "\n",
    "    # https://github.com/thuwuyinjun/DeltaGrad/blob/master/src/main_delete.py#L1202\n",
    "    # https://github.com/thuwuyinjun/DeltaGrad/blob/master/src/main_delete.py#L815\n",
    "\n",
    "    # https://github.com/thuwuyinjun/DeltaGrad/blob/master/src/main_delete.py#L819\n",
    "    para = list(model.parameters())\n",
    "    # https://github.com/thuwuyinjun/DeltaGrad/blob/master/src/main_delete.py#L826\n",
    "    full_shape_list, shape_list, total_shape_size = get_model_para_shape_list(model.parameters())\n",
    "\n",
    "    # https://github.com/thuwuyinjun/DeltaGrad/blob/master/src/main_delete.py#L836\n",
    "    S_k_list = deque()\n",
    "    Y_k_list = deque()\n",
    "    \n",
    "    train_times = list()\n",
    "    \n",
    "    train_accs, test_accs, forget_accs = list(), list(), list()\n",
    "    \n",
    "    for epoch in range(EPOCHS):    \n",
    "        \n",
    "        # train\n",
    "        \n",
    "        train_time = 0\n",
    "        \n",
    "        start_time = time.time()\n",
    "\n",
    "        model.train()\n",
    "        for i in range(num_steps):\n",
    "\n",
    "            # https://github.com/thuwuyinjun/DeltaGrad/blob/master/src/main_delete.py#L890\n",
    "\n",
    "            batch_remaining_X = train_x[train_batch_size*i:train_batch_size*(i+1)].cuda()\n",
    "            batch_remaining_Y = train_y[train_batch_size*i:train_batch_size*(i+1)].cuda()\n",
    "\n",
    "            # https://github.com/thuwuyinjun/DeltaGrad/blob/master/src/main_delete.py#L930\n",
    "\n",
    "            batch_delta_X = forget_x[forget_batch_size*i:forget_batch_size*(i+1)].cuda()\n",
    "            batch_delta_Y = forget_y[forget_batch_size*i:forget_batch_size*(i+1)].cuda()\n",
    "\n",
    "            # https://github.com/thuwuyinjun/DeltaGrad/blob/master/src/main_delete.py#L895\n",
    "\n",
    "            curr_matched_ids_size = batch_delta_X.shape[0]\n",
    "\n",
    "            # https://github.com/thuwuyinjun/DeltaGrad/blob/master/src/model_train.py#L18\n",
    "            \n",
    "            net_optimizer.zero_grad()\n",
    "            output = net(torch.concat([batch_remaining_X, batch_delta_X], dim=0))\n",
    "            loss = criterion(output, torch.concat([batch_remaining_Y, batch_delta_Y], dim=0))\n",
    "            loss.backward()\n",
    "            net_optimizer.step()\n",
    "\n",
    "            gradient_list = []\n",
    "            para_list = []\n",
    "            append_gradient_list(gradient_list, None, para_list, net, None, is_GPU, device)\n",
    "\n",
    "            para_list_tensor, grad_list_tensor = post_processing_gradien_para_list_all_epochs(para_list, gradient_list)\n",
    "            para_list_tensor, grad_list_tensor = para_list_tensor.cuda(), grad_list_tensor.cuda()\n",
    "                \n",
    "            # https://github.com/thuwuyinjun/DeltaGrad/blob/master/src/main_delete.py#L927\n",
    "\n",
    "            if epoch < INIT_EPOCHS:\n",
    "\n",
    "                para, _, init_hessian_para_prod, theta_k = explicit_iters(\n",
    "                    batch_delta_X, batch_delta_Y, batch_remaining_X, batch_remaining_Y, \n",
    "                    curr_matched_ids_size, model, para, epoch, i, M+1, S_k_list, Y_k_list, LR, WD, \n",
    "                    grad_list_tensor, grad_list_tensor, 0, full_shape_list, shape_list, \n",
    "                    is_GPU, device, \n",
    "                    criterion, optimizer, None, None\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                \n",
    "                '''use l-bfgs algorithm to evaluate the gradients'''\n",
    "\n",
    "                # https://github.com/thuwuyinjun/DeltaGrad/blob/master/src/main_delete.py#L952\n",
    "                \n",
    "                init_model(model, para)\n",
    "\n",
    "                compute_derivative_one_more_step(model, batch_delta_X, batch_delta_Y, criterion, optimizer)\n",
    "                \n",
    "                gradient_dual = get_all_gradient(model)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                \n",
    "                    vec_para_diff = torch.t((get_all_vectorized_parameters1(para) - para_list_tensor))\n",
    "                    \n",
    "                    if (epoch - INIT_EPOCHS) / PERIOD >= 1:\n",
    "                        if (epoch - INIT_EPOCHS) % PERIOD == 0:\n",
    "                            zero_mat_dim, curr_Y_k, curr_S_k, sigma_k, mat_prime = prepare_hessian_vec_prod0_3(list(S_k_list)[1:], list(Y_k_list)[1:], i, INIT_EPOCHS, M, is_GPU, device)\n",
    "                            \n",
    "                            mat = np.linalg.inv(mat_prime.cpu().numpy())\n",
    "                            mat = torch.from_numpy(mat)\n",
    "                            mat = mat.to(device)\n",
    "                            \n",
    "                        hessian_para_prod = compute_approx_hessian_vector_prod_with_prepared_terms1(zero_mat_dim, curr_Y_k, curr_S_k, sigma_k, mat, vec_para_diff, is_GPU, device)\n",
    "                        \n",
    "                    else:\n",
    "                        '''S_k_list, Y_k_list, v_vec, k, is_GPU, device'''\n",
    "                        hessian_para_prod, zero_mat_dim, curr_Y_k, curr_S_k, sigma_k, mat_prime = cal_approx_hessian_vec_prod0_3(list(S_k_list)[1:], list(Y_k_list)[1:], vec_para_diff, M, is_GPU, device)\n",
    "                    \n",
    "                    is_positive, final_gradient_list = compute_grad_final3(\n",
    "                        get_all_vectorized_parameters1(para), torch.t(hessian_para_prod), \n",
    "                        get_all_vectorized_parameters1(gradient_dual), \n",
    "                        grad_list_tensor, para_list_tensor, \n",
    "                        batch_remaining_X.shape[0] + curr_matched_ids_size, curr_matched_ids_size, \n",
    "                        LR, WD, is_GPU, device\n",
    "                    )\n",
    "                        \n",
    "                    vec_para = update_para_final2(para, final_gradient_list, LR)\n",
    "                    \n",
    "                    para = get_devectorized_parameters(vec_para, full_shape_list, shape_list)\n",
    "                \n",
    "\n",
    "        \n",
    "        train_time += time.time() - start_time\n",
    "            \n",
    "        train_times.append(train_time)\n",
    "        \n",
    "        # test\n",
    "            \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            #\n",
    "            \n",
    "            x, y = train_set.tensors\n",
    "            \n",
    "            accs = list()\n",
    "            \n",
    "            for i in range(0, x.shape[0], BATCH_SIZE):\n",
    "            \n",
    "                output = model(x[i:i+BATCH_SIZE].cuda())\n",
    "\n",
    "                predicted = torch.argmax(output.data, dim=-1)\n",
    "                accs.append((predicted == y[i:i+BATCH_SIZE].cuda()).float().mean().detach().cpu().numpy())\n",
    "            \n",
    "            train_accs.append(np.mean(accs))\n",
    "            \n",
    "            #\n",
    "            \n",
    "            x, y = test_set.tensors\n",
    "            \n",
    "            accs = list()\n",
    "            \n",
    "            for i in range(0, x.shape[0], BATCH_SIZE):\n",
    "            \n",
    "                output = model(x[i:i+BATCH_SIZE].cuda())\n",
    "\n",
    "                predicted = torch.argmax(output.data, dim=-1)\n",
    "                accs.append((predicted == y[i:i+BATCH_SIZE].cuda()).float().mean().detach().cpu().numpy())\n",
    "            \n",
    "            test_accs.append(np.mean(accs))\n",
    "            \n",
    "            #\n",
    "\n",
    "            x, y = forget_set.tensors\n",
    "\n",
    "            for i in range(0, x.shape[0], BATCH_SIZE):\n",
    "\n",
    "                output = model(x[i:i+BATCH_SIZE].cuda())\n",
    "\n",
    "                predicted = torch.argmax(output.data, dim=-1)\n",
    "                accs.append((predicted == y[i:i+BATCH_SIZE].cuda()).float().mean().detach().cpu().numpy())\n",
    "\n",
    "            forget_accs.append(np.mean(accs))\n",
    "        \n",
    "        # save\n",
    "        torch.save(model.state_dict(), os.path.join(save_dir, f'{(epoch+1):03d}.pt'))\n",
    "\n",
    "    return train_times, train_accs, test_accs, forget_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "147b38b7-2f8b-44e9-88cd-572bd54ee2f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a27a177deb8d41b6a45ad7efc3b109ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = list()\n",
    "\n",
    "for percentage in tqdm(PERCENTAGES):\n",
    "\n",
    "    # https://github.com/thuwuyinjun/DeltaGrad/blob/master/src/main_delete.py#L1124\n",
    "    # https://github.com/thuwuyinjun/DeltaGrad/blob/master/src/Models/DNN_single.py#L47\n",
    "    # remove softmax\n",
    "    model = DGCNN().cuda()\n",
    "        \n",
    "    # https://github.com/thuwuyinjun/DeltaGrad/blob/master/src/main_delete.py#L1133\n",
    "    model.load_state_dict(torch.load('./weights/init.pt'))\n",
    "    \n",
    "    train_set, test_set, forget_set = load_deleted_dataset(DATA_DIR, percentage)\n",
    "    \n",
    "    train_times, train_accs, test_accs, forget_accs = fit(model, f'weights/DeltaGrad/{percentage}', train_set, test_set, forget_set)\n",
    "    \n",
    "    df = pd.DataFrame(zip(train_times, train_accs, test_accs, forget_accs), columns=['train_time', 'train_acc', 'test_acc', 'forget_acc'])\n",
    "    df['epoch'] = range(1, EPOCHS+1)\n",
    "    df['percentage'] = percentage\n",
    "    \n",
    "    results.append(df)\n",
    "\n",
    "results = pd.concat(results).set_index(['percentage', 'epoch'])\n",
    "\n",
    "results.to_csv(f'results/DeltaGrad.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "9b6eda0e-5b3c-432a-8be8-7e85dfbbf08a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>train_time</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>test_acc</th>\n",
       "      <th>forget_acc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>percentage</th>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th>1</th>\n",
       "      <td>2.047155</td>\n",
       "      <td>0.154429</td>\n",
       "      <td>0.145484</td>\n",
       "      <td>0.146046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.026246</td>\n",
       "      <td>0.097077</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.098046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.037493</td>\n",
       "      <td>0.097077</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.098046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.052048</td>\n",
       "      <td>0.097077</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.098046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.304612</td>\n",
       "      <td>0.097077</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.098046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">10</th>\n",
       "      <th>1</th>\n",
       "      <td>2.238466</td>\n",
       "      <td>0.098714</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.098487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.210558</td>\n",
       "      <td>0.098714</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.098487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.178865</td>\n",
       "      <td>0.098714</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.098487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.166264</td>\n",
       "      <td>0.098714</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.098487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.423311</td>\n",
       "      <td>0.098714</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.098487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">20</th>\n",
       "      <th>1</th>\n",
       "      <td>2.288216</td>\n",
       "      <td>0.098268</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.099570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.250998</td>\n",
       "      <td>0.098268</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.099570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.392770</td>\n",
       "      <td>0.098268</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.099570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.224242</td>\n",
       "      <td>0.098268</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.099570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.543263</td>\n",
       "      <td>0.098268</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.099570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">30</th>\n",
       "      <th>1</th>\n",
       "      <td>2.277614</td>\n",
       "      <td>0.100237</td>\n",
       "      <td>0.101054</td>\n",
       "      <td>0.103394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.361688</td>\n",
       "      <td>0.097726</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.098575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.420295</td>\n",
       "      <td>0.097726</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.098575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.323048</td>\n",
       "      <td>0.097726</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.098575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.555659</td>\n",
       "      <td>0.097726</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.098575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">40</th>\n",
       "      <th>1</th>\n",
       "      <td>2.299664</td>\n",
       "      <td>0.098573</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.098817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.473107</td>\n",
       "      <td>0.098573</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.098817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.425554</td>\n",
       "      <td>0.098573</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.098817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.427842</td>\n",
       "      <td>0.098573</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.098817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.672208</td>\n",
       "      <td>0.098573</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.098817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">50</th>\n",
       "      <th>1</th>\n",
       "      <td>2.262221</td>\n",
       "      <td>0.098578</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.098397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.550789</td>\n",
       "      <td>0.098578</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.098397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.572583</td>\n",
       "      <td>0.098578</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.098397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.537967</td>\n",
       "      <td>0.098578</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.098397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.799056</td>\n",
       "      <td>0.098578</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.098397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">60</th>\n",
       "      <th>1</th>\n",
       "      <td>2.231710</td>\n",
       "      <td>0.099406</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.098204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.590240</td>\n",
       "      <td>0.099406</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.098204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.592604</td>\n",
       "      <td>0.099406</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.098204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.598969</td>\n",
       "      <td>0.099406</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.098204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.881815</td>\n",
       "      <td>0.099406</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.098204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">70</th>\n",
       "      <th>1</th>\n",
       "      <td>2.212235</td>\n",
       "      <td>0.098867</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.100237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.718171</td>\n",
       "      <td>0.098867</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.100237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.787418</td>\n",
       "      <td>0.098867</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.100237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.676520</td>\n",
       "      <td>0.098867</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.100237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.007325</td>\n",
       "      <td>0.098867</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.100237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">80</th>\n",
       "      <th>1</th>\n",
       "      <td>2.306676</td>\n",
       "      <td>0.099429</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.098471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.799172</td>\n",
       "      <td>0.099429</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.098471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.808850</td>\n",
       "      <td>0.099429</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.098471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.760643</td>\n",
       "      <td>0.099429</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.098471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.114192</td>\n",
       "      <td>0.099429</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.098471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">90</th>\n",
       "      <th>1</th>\n",
       "      <td>2.344773</td>\n",
       "      <td>0.097831</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.098758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.920404</td>\n",
       "      <td>0.097831</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.098758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.940986</td>\n",
       "      <td>0.097831</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.098758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.897836</td>\n",
       "      <td>0.097831</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.098758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.301194</td>\n",
       "      <td>0.097831</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.098758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">99</th>\n",
       "      <th>1</th>\n",
       "      <td>2.159054</td>\n",
       "      <td>0.096667</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.097237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.681888</td>\n",
       "      <td>0.096667</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.097237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.682208</td>\n",
       "      <td>0.096667</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.097237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.763195</td>\n",
       "      <td>0.096667</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.097237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.049847</td>\n",
       "      <td>0.096667</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.097237</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  train_time  train_acc  test_acc  forget_acc\n",
       "percentage epoch                                             \n",
       "1          1        2.047155   0.154429  0.145484    0.146046\n",
       "           2        2.026246   0.097077  0.098184    0.098046\n",
       "           3        2.037493   0.097077  0.098184    0.098046\n",
       "           4        2.052048   0.097077  0.098184    0.098046\n",
       "           5        1.304612   0.097077  0.098184    0.098046\n",
       "10         1        2.238466   0.098714  0.098184    0.098487\n",
       "           2        2.210558   0.098714  0.098184    0.098487\n",
       "           3        2.178865   0.098714  0.098184    0.098487\n",
       "           4        2.166264   0.098714  0.098184    0.098487\n",
       "           5        1.423311   0.098714  0.098184    0.098487\n",
       "20         1        2.288216   0.098268  0.098184    0.099570\n",
       "           2        2.250998   0.098268  0.098184    0.099570\n",
       "           3        2.392770   0.098268  0.098184    0.099570\n",
       "           4        2.224242   0.098268  0.098184    0.099570\n",
       "           5        1.543263   0.098268  0.098184    0.099570\n",
       "30         1        2.277614   0.100237  0.101054    0.103394\n",
       "           2        2.361688   0.097726  0.098184    0.098575\n",
       "           3        2.420295   0.097726  0.098184    0.098575\n",
       "           4        2.323048   0.097726  0.098184    0.098575\n",
       "           5        1.555659   0.097726  0.098184    0.098575\n",
       "40         1        2.299664   0.098573  0.098184    0.098817\n",
       "           2        2.473107   0.098573  0.098184    0.098817\n",
       "           3        2.425554   0.098573  0.098184    0.098817\n",
       "           4        2.427842   0.098573  0.098184    0.098817\n",
       "           5        1.672208   0.098573  0.098184    0.098817\n",
       "50         1        2.262221   0.098578  0.098184    0.098397\n",
       "           2        2.550789   0.098578  0.098184    0.098397\n",
       "           3        2.572583   0.098578  0.098184    0.098397\n",
       "           4        2.537967   0.098578  0.098184    0.098397\n",
       "           5        1.799056   0.098578  0.098184    0.098397\n",
       "60         1        2.231710   0.099406  0.098184    0.098204\n",
       "           2        2.590240   0.099406  0.098184    0.098204\n",
       "           3        2.592604   0.099406  0.098184    0.098204\n",
       "           4        2.598969   0.099406  0.098184    0.098204\n",
       "           5        1.881815   0.099406  0.098184    0.098204\n",
       "70         1        2.212235   0.098867  0.098184    0.100237\n",
       "           2        2.718171   0.098867  0.098184    0.100237\n",
       "           3        2.787418   0.098867  0.098184    0.100237\n",
       "           4        2.676520   0.098867  0.098184    0.100237\n",
       "           5        2.007325   0.098867  0.098184    0.100237\n",
       "80         1        2.306676   0.099429  0.098184    0.098471\n",
       "           2        2.799172   0.099429  0.098184    0.098471\n",
       "           3        2.808850   0.099429  0.098184    0.098471\n",
       "           4        2.760643   0.099429  0.098184    0.098471\n",
       "           5        2.114192   0.099429  0.098184    0.098471\n",
       "90         1        2.344773   0.097831  0.098184    0.098758\n",
       "           2        2.920404   0.097831  0.098184    0.098758\n",
       "           3        2.940986   0.097831  0.098184    0.098758\n",
       "           4        2.897836   0.097831  0.098184    0.098758\n",
       "           5        2.301194   0.097831  0.098184    0.098758\n",
       "99         1        2.159054   0.096667  0.098184    0.097237\n",
       "           2        2.681888   0.096667  0.098184    0.097237\n",
       "           3        2.682208   0.096667  0.098184    0.097237\n",
       "           4        2.763195   0.096667  0.098184    0.097237\n",
       "           5        2.049847   0.096667  0.098184    0.097237"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542d9cae-795a-4bc7-8e05-883e2315f4e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
