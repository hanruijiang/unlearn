{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b703d374-8ed7-4fbc-9241-543d4c3883c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "\n",
    "import torch\n",
    "\n",
    "import copy\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13c94faf-8d6c-44aa-9618-7192902b1933",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_original_dataset, load_deleted_dataset\n",
    "from models import CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19d524cc-f9e7-4806-af84-a4429d0e47e0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = 'Datasets/Features/'\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5\n",
    "PERCENTAGES = [1, 10, 20, 30, 40, 50, 60, 70, 80, 90, 99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48a614e7-46f8-451b-aa48-9624b48efe77",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs('results/kpriors', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d647010e-c768-42fb-adf9-af6d0a83e0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = True\n",
    "\n",
    "# https://github.com/team-approx-bayes/kpriors/blob/main/main.py#L71\n",
    "learning_rate = 0.005\n",
    "\n",
    "# https://github.com/team-approx-bayes/kpriors/blob/main/main.py#L20\n",
    "adaptation_task = 'remove_data'\n",
    "\n",
    "# https://github.com/team-approx-bayes/kpriors/blob/main/main.py#L83\n",
    "prior_prec = 5\n",
    "\n",
    "# https://github.com/team-approx-bayes/kpriors/blob/main/main.py#L31\n",
    "adaptation_methods = ['Replay','K-priors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2108aae4-de7a-4176-bc15-369c134f91e8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath('./libraries/kpriors/'))\n",
    "\n",
    "# https://github.com/team-approx-bayes/kpriors/blob/main/main.py#L114\n",
    "from adamreg import AdamReg\n",
    "import utils\n",
    "import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00892bc0-072b-4e49-b728-feb52c23d270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/team-approx-bayes/kpriors/blob/main/models.py#L75\n",
    "\n",
    "# Return all parameters as a vector\n",
    "def return_parameters(model):\n",
    "    num_params = sum([np.prod(p.size()) for p in model.parameters()])\n",
    "    means = torch.zeros(num_params)\n",
    "\n",
    "    start_ind = 0\n",
    "    for p in model.parameters():\n",
    "        num = np.prod(p.size())\n",
    "        means[start_ind:start_ind+num] = p.data.reshape(-1)\n",
    "        start_ind += num\n",
    "\n",
    "    return means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c22c42d-40af-4ef6-b18c-103c2678403b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fit(base_model, save_dir, train_set, test_set, forget_set):\n",
    "    \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # https://github.com/team-approx-bayes/kpriors/blob/main/train.py#L10\n",
    "    error = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    # prepare model\n",
    "    \n",
    "    # https://github.com/team-approx-bayes/kpriors/blob/main/main.py#L159\n",
    "    model = copy.deepcopy(base_model)\n",
    "    optimiser = AdamReg(model, lr=learning_rate, weight_decay=prior_prec)\n",
    "    optimiser.previous_weights = return_parameters(base_model)\n",
    "\n",
    "    def select_memory_points(base_train_data, additional_memory_data):\n",
    "        \n",
    "        # Select points\n",
    "        # https://github.com/team-approx-bayes/kpriors/blob/main/main.py#L144\n",
    "        memory_points = {}\n",
    "        # https://github.com/team-approx-bayes/kpriors/blob/main/utils.py#L63\n",
    "        # https://github.com/team-approx-bayes/kpriors/blob/main/utils.py#L72\n",
    "        memory_points['inputs'] = torch.cat((base_train_data[0], additional_memory_data[0]))\n",
    "        memory_points['true_labels'] = torch.cat((base_train_data[1], additional_memory_data[1]))\n",
    "        if use_cuda:\n",
    "            memory_points['inputs'] = memory_points['inputs'].cuda()\n",
    "        # https://github.com/team-approx-bayes/kpriors/blob/main/utils.py#L79\n",
    "        memory_points['soft_labels'] = torch.softmax(base_model.forward(memory_points['inputs']), dim=-1)\n",
    "        \n",
    "        \n",
    "        # Soft labels in K-priors, hard (true) labels in Replay\n",
    "        if adaptation_method == \"K-priors\":\n",
    "            memory_points['labels'] = memory_points['soft_labels']\n",
    "        elif adaptation_method == \"Replay\":\n",
    "            memory_points['labels'] = torch.nn.functional.one_hot(memory_points['true_labels'], num_classes=10)\n",
    "        \n",
    "        # Store past memory labels\n",
    "        # https://github.com/team-approx-bayes/kpriors/blob/main/main.py#L230\n",
    "        optimiser.memory_labels = memory_points['labels']\n",
    "\n",
    "        return memory_points\n",
    "\n",
    "    \n",
    "    # https://github.com/team-approx-bayes/kpriors/blob/main/main.py#L218\n",
    "    optimiser.prior_prec_old = prior_prec\n",
    "    remove_data_bool = True\n",
    "\n",
    "    # https://github.com/team-approx-bayes/kpriors/blob/main/main.py#L49\n",
    "    train_batch_size = int(np.ceil(BATCH_SIZE * len(train_set) / (len(train_set) + len(forget_set))))\n",
    "    forget_batch_size = int(np.ceil(BATCH_SIZE * len(forget_set) / (len(train_set) + len(forget_set))))\n",
    "\n",
    "    num_steps = min(len(train_set) // train_batch_size, len(forget_set) // forget_batch_size)\n",
    "\n",
    "    train_x, train_y = train_set.tensors[0], train_set.tensors[1]\n",
    "    forget_x, forget_y = forget_set.tensors[0], forget_set.tensors[1]\n",
    "    \n",
    "    train_times = list()\n",
    "    \n",
    "    train_accs, test_accs, forget_accs = list(), list(), list()\n",
    "    \n",
    "    for epoch in range(EPOCHS):    \n",
    "        \n",
    "        # train\n",
    "        \n",
    "        train_time = 0\n",
    "        \n",
    "        start_time = time.time()\n",
    "\n",
    "        model.train()\n",
    "        for i in range(num_steps):\n",
    "            \n",
    "            # If remove_data task, then store the removed points too, for both K-priors and Replay\n",
    "            \n",
    "            # https://github.com/team-approx-bayes/kpriors/blob/main/main.py#L140\n",
    "            # https://github.com/team-approx-bayes/kpriors/blob/main/main.py#L134\n",
    "            base_train_data = (train_x[train_batch_size*i:train_batch_size*(i+1)], train_y[train_batch_size*i:train_batch_size*(i+1)])\n",
    "            additional_memory_data = (forget_x[forget_batch_size*i:forget_batch_size*(i+1)], forget_y[forget_batch_size*i:forget_batch_size*(i+1)])\n",
    "            \n",
    "            # Load data for adaptation task\n",
    "            # https://github.com/team-approx-bayes/kpriors/blob/main/main.py#L148\n",
    "            # https://github.com/team-approx-bayes/kpriors/blob/main/data_generators.py#L67\n",
    "            adapt_train_data = additional_memory_data\n",
    "    \n",
    "            memory_points = select_memory_points(base_train_data, additional_memory_data)\n",
    "                \n",
    "            # Train model\n",
    "            # https://github.com/team-approx-bayes/kpriors/blob/main/main.py#L235\n",
    "            \n",
    "            train.train_model(\n",
    "                model, optimiser, adapt_train_data, \n",
    "                num_epochs=1, # one epoch\n",
    "                memory_data=memory_points,\n",
    "                adaptation_method=adaptation_method, \n",
    "                remove_data_bool=remove_data_bool, \n",
    "                use_cuda=use_cuda\n",
    "            )\n",
    "        \n",
    "        train_time += time.time() - start_time\n",
    "            \n",
    "        train_times.append(train_time)\n",
    "        \n",
    "        # test\n",
    "            \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            #\n",
    "            \n",
    "            x, y = train_set.tensors\n",
    "            \n",
    "            accs = list()\n",
    "            \n",
    "            for i in range(0, x.shape[0], BATCH_SIZE):\n",
    "            \n",
    "                output = model(x[i:i+BATCH_SIZE].cuda())\n",
    "\n",
    "                predicted = torch.argmax(output.data, dim=-1)\n",
    "                accs.append((predicted == y[i:i+BATCH_SIZE].cuda()).float().mean().detach().cpu().numpy())\n",
    "            \n",
    "            train_accs.append(np.mean(accs))\n",
    "            \n",
    "            #\n",
    "            \n",
    "            x, y = test_set.tensors\n",
    "            \n",
    "            accs = list()\n",
    "            \n",
    "            for i in range(0, x.shape[0], BATCH_SIZE):\n",
    "            \n",
    "                output = model(x[i:i+BATCH_SIZE].cuda())\n",
    "\n",
    "                predicted = torch.argmax(output.data, dim=-1)\n",
    "                accs.append((predicted == y[i:i+BATCH_SIZE].cuda()).float().mean().detach().cpu().numpy())\n",
    "            \n",
    "            test_accs.append(np.mean(accs))\n",
    "            \n",
    "            #\n",
    "\n",
    "            x, y = forget_set.tensors\n",
    "\n",
    "            for i in range(0, x.shape[0], BATCH_SIZE):\n",
    "\n",
    "                output = model(x[i:i+BATCH_SIZE].cuda())\n",
    "\n",
    "                predicted = torch.argmax(output.data, dim=-1)\n",
    "                accs.append((predicted == y[i:i+BATCH_SIZE].cuda()).float().mean().detach().cpu().numpy())\n",
    "\n",
    "            forget_accs.append(np.mean(accs))\n",
    "        \n",
    "        # save\n",
    "        torch.save(model.state_dict(), os.path.join(save_dir, f'{(epoch+1):03d}.pt'))\n",
    "\n",
    "    return train_times, train_accs, test_accs, forget_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ad852eb-cb3d-4f0c-99ec-606bc20cfd4a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eded0e9bd6a45409f18d5520324da29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b58da5648c54c61aee65f2262cf03d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for adaptation_method in adaptation_methods:\n",
    "\n",
    "    results = list()\n",
    "    \n",
    "    for percentage in tqdm(PERCENTAGES):\n",
    "        \n",
    "        model = CNN().cuda()\n",
    "        \n",
    "        model.load_state_dict(torch.load('./weights/original/005.pt'))\n",
    "        \n",
    "        train_set, test_set, forget_set = load_deleted_dataset(DATA_DIR, percentage)\n",
    "        \n",
    "        train_times, train_accs, test_accs, forget_accs = fit(model, f'weights/kpriors/{adaptation_method}/{percentage}', train_set, test_set, forget_set)\n",
    "        \n",
    "        df = pd.DataFrame(zip(train_times, train_accs, test_accs, forget_accs), columns=['train_time', 'train_acc', 'test_acc', 'forget_acc'])\n",
    "        df['epoch'] = range(1, EPOCHS+1)\n",
    "        df['percentage'] = percentage\n",
    "        \n",
    "        results.append(df)\n",
    "\n",
    "    results = pd.concat(results).set_index(['percentage', 'epoch'])\n",
    "    \n",
    "    results.to_csv(f'results/kpriors/{adaptation_method}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7d0a870-9f24-4a6c-acc8-e75b640da0c4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>train_time</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>test_acc</th>\n",
       "      <th>forget_acc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>percentage</th>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th>1</th>\n",
       "      <td>6.384204</td>\n",
       "      <td>0.970904</td>\n",
       "      <td>0.971246</td>\n",
       "      <td>0.971386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.487081</td>\n",
       "      <td>0.957694</td>\n",
       "      <td>0.959565</td>\n",
       "      <td>0.960185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.516479</td>\n",
       "      <td>0.967269</td>\n",
       "      <td>0.967851</td>\n",
       "      <td>0.968373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.594802</td>\n",
       "      <td>0.971931</td>\n",
       "      <td>0.972943</td>\n",
       "      <td>0.973080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.265817</td>\n",
       "      <td>0.956617</td>\n",
       "      <td>0.956969</td>\n",
       "      <td>0.957549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">10</th>\n",
       "      <th>1</th>\n",
       "      <td>16.795093</td>\n",
       "      <td>0.975544</td>\n",
       "      <td>0.976138</td>\n",
       "      <td>0.975923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16.587360</td>\n",
       "      <td>0.964085</td>\n",
       "      <td>0.966354</td>\n",
       "      <td>0.966567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.880488</td>\n",
       "      <td>0.966677</td>\n",
       "      <td>0.969649</td>\n",
       "      <td>0.969561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.011639</td>\n",
       "      <td>0.944887</td>\n",
       "      <td>0.945887</td>\n",
       "      <td>0.947605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>16.907945</td>\n",
       "      <td>0.953384</td>\n",
       "      <td>0.954473</td>\n",
       "      <td>0.956649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">20</th>\n",
       "      <th>1</th>\n",
       "      <td>20.320035</td>\n",
       "      <td>0.959125</td>\n",
       "      <td>0.960763</td>\n",
       "      <td>0.960074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.417346</td>\n",
       "      <td>0.954896</td>\n",
       "      <td>0.954772</td>\n",
       "      <td>0.954079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19.539327</td>\n",
       "      <td>0.963417</td>\n",
       "      <td>0.965954</td>\n",
       "      <td>0.963254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19.620408</td>\n",
       "      <td>0.955312</td>\n",
       "      <td>0.957768</td>\n",
       "      <td>0.955532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>19.559124</td>\n",
       "      <td>0.948521</td>\n",
       "      <td>0.950779</td>\n",
       "      <td>0.949173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">30</th>\n",
       "      <th>1</th>\n",
       "      <td>20.140157</td>\n",
       "      <td>0.899419</td>\n",
       "      <td>0.902556</td>\n",
       "      <td>0.899579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20.996589</td>\n",
       "      <td>0.733958</td>\n",
       "      <td>0.734425</td>\n",
       "      <td>0.733626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21.035202</td>\n",
       "      <td>0.959206</td>\n",
       "      <td>0.959864</td>\n",
       "      <td>0.959511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.399345</td>\n",
       "      <td>0.910653</td>\n",
       "      <td>0.912640</td>\n",
       "      <td>0.911102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20.747788</td>\n",
       "      <td>0.914985</td>\n",
       "      <td>0.915635</td>\n",
       "      <td>0.911672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">40</th>\n",
       "      <th>1</th>\n",
       "      <td>21.183923</td>\n",
       "      <td>0.828944</td>\n",
       "      <td>0.828574</td>\n",
       "      <td>0.830433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21.335480</td>\n",
       "      <td>0.766556</td>\n",
       "      <td>0.765176</td>\n",
       "      <td>0.767168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21.301043</td>\n",
       "      <td>0.903083</td>\n",
       "      <td>0.904453</td>\n",
       "      <td>0.907044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.913690</td>\n",
       "      <td>0.931222</td>\n",
       "      <td>0.930611</td>\n",
       "      <td>0.930503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>21.012361</td>\n",
       "      <td>0.876861</td>\n",
       "      <td>0.878494</td>\n",
       "      <td>0.878939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">50</th>\n",
       "      <th>1</th>\n",
       "      <td>21.750778</td>\n",
       "      <td>0.355410</td>\n",
       "      <td>0.352137</td>\n",
       "      <td>0.350195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22.057901</td>\n",
       "      <td>0.859275</td>\n",
       "      <td>0.861122</td>\n",
       "      <td>0.856540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21.852227</td>\n",
       "      <td>0.832922</td>\n",
       "      <td>0.834165</td>\n",
       "      <td>0.831310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22.523047</td>\n",
       "      <td>0.892024</td>\n",
       "      <td>0.892572</td>\n",
       "      <td>0.890338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>21.850565</td>\n",
       "      <td>0.757696</td>\n",
       "      <td>0.770966</td>\n",
       "      <td>0.758518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">60</th>\n",
       "      <th>1</th>\n",
       "      <td>20.742844</td>\n",
       "      <td>0.814292</td>\n",
       "      <td>0.813898</td>\n",
       "      <td>0.810349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21.556610</td>\n",
       "      <td>0.379375</td>\n",
       "      <td>0.381190</td>\n",
       "      <td>0.378412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21.468966</td>\n",
       "      <td>0.641542</td>\n",
       "      <td>0.652456</td>\n",
       "      <td>0.643428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21.421669</td>\n",
       "      <td>0.865042</td>\n",
       "      <td>0.866014</td>\n",
       "      <td>0.861461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>21.288592</td>\n",
       "      <td>0.816833</td>\n",
       "      <td>0.817292</td>\n",
       "      <td>0.812044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">70</th>\n",
       "      <th>1</th>\n",
       "      <td>20.846441</td>\n",
       "      <td>0.594694</td>\n",
       "      <td>0.594449</td>\n",
       "      <td>0.587023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21.131107</td>\n",
       "      <td>0.778031</td>\n",
       "      <td>0.766573</td>\n",
       "      <td>0.768796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20.579022</td>\n",
       "      <td>0.448324</td>\n",
       "      <td>0.444289</td>\n",
       "      <td>0.439998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.767252</td>\n",
       "      <td>0.508825</td>\n",
       "      <td>0.508087</td>\n",
       "      <td>0.503056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20.502395</td>\n",
       "      <td>0.434780</td>\n",
       "      <td>0.428315</td>\n",
       "      <td>0.431581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">80</th>\n",
       "      <th>1</th>\n",
       "      <td>19.752287</td>\n",
       "      <td>0.184167</td>\n",
       "      <td>0.183307</td>\n",
       "      <td>0.180312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.717808</td>\n",
       "      <td>0.258417</td>\n",
       "      <td>0.261282</td>\n",
       "      <td>0.258911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20.207478</td>\n",
       "      <td>0.105667</td>\n",
       "      <td>0.113419</td>\n",
       "      <td>0.113813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21.309026</td>\n",
       "      <td>0.322833</td>\n",
       "      <td>0.325978</td>\n",
       "      <td>0.323997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>19.911584</td>\n",
       "      <td>0.445250</td>\n",
       "      <td>0.446685</td>\n",
       "      <td>0.441395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">90</th>\n",
       "      <th>1</th>\n",
       "      <td>17.469472</td>\n",
       "      <td>0.525266</td>\n",
       "      <td>0.519369</td>\n",
       "      <td>0.515898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.320393</td>\n",
       "      <td>0.113863</td>\n",
       "      <td>0.104433</td>\n",
       "      <td>0.105541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18.338756</td>\n",
       "      <td>0.106715</td>\n",
       "      <td>0.114617</td>\n",
       "      <td>0.114318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18.129450</td>\n",
       "      <td>0.206283</td>\n",
       "      <td>0.214557</td>\n",
       "      <td>0.215642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>17.954962</td>\n",
       "      <td>0.166390</td>\n",
       "      <td>0.170627</td>\n",
       "      <td>0.165683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">99</th>\n",
       "      <th>1</th>\n",
       "      <td>6.548265</td>\n",
       "      <td>0.153509</td>\n",
       "      <td>0.159545</td>\n",
       "      <td>0.157805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.961410</td>\n",
       "      <td>0.170504</td>\n",
       "      <td>0.166733</td>\n",
       "      <td>0.165726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.759411</td>\n",
       "      <td>0.118969</td>\n",
       "      <td>0.126098</td>\n",
       "      <td>0.123574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.917278</td>\n",
       "      <td>0.112939</td>\n",
       "      <td>0.095847</td>\n",
       "      <td>0.098085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.966128</td>\n",
       "      <td>0.075110</td>\n",
       "      <td>0.091154</td>\n",
       "      <td>0.093361</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  train_time  train_acc  test_acc  forget_acc\n",
       "percentage epoch                                             \n",
       "1          1        6.384204   0.970904  0.971246    0.971386\n",
       "           2        6.487081   0.957694  0.959565    0.960185\n",
       "           3        6.516479   0.967269  0.967851    0.968373\n",
       "           4        6.594802   0.971931  0.972943    0.973080\n",
       "           5        6.265817   0.956617  0.956969    0.957549\n",
       "10         1       16.795093   0.975544  0.976138    0.975923\n",
       "           2       16.587360   0.964085  0.966354    0.966567\n",
       "           3       16.880488   0.966677  0.969649    0.969561\n",
       "           4       17.011639   0.944887  0.945887    0.947605\n",
       "           5       16.907945   0.953384  0.954473    0.956649\n",
       "20         1       20.320035   0.959125  0.960763    0.960074\n",
       "           2       19.417346   0.954896  0.954772    0.954079\n",
       "           3       19.539327   0.963417  0.965954    0.963254\n",
       "           4       19.620408   0.955312  0.957768    0.955532\n",
       "           5       19.559124   0.948521  0.950779    0.949173\n",
       "30         1       20.140157   0.899419  0.902556    0.899579\n",
       "           2       20.996589   0.733958  0.734425    0.733626\n",
       "           3       21.035202   0.959206  0.959864    0.959511\n",
       "           4       20.399345   0.910653  0.912640    0.911102\n",
       "           5       20.747788   0.914985  0.915635    0.911672\n",
       "40         1       21.183923   0.828944  0.828574    0.830433\n",
       "           2       21.335480   0.766556  0.765176    0.767168\n",
       "           3       21.301043   0.903083  0.904453    0.907044\n",
       "           4       20.913690   0.931222  0.930611    0.930503\n",
       "           5       21.012361   0.876861  0.878494    0.878939\n",
       "50         1       21.750778   0.355410  0.352137    0.350195\n",
       "           2       22.057901   0.859275  0.861122    0.856540\n",
       "           3       21.852227   0.832922  0.834165    0.831310\n",
       "           4       22.523047   0.892024  0.892572    0.890338\n",
       "           5       21.850565   0.757696  0.770966    0.758518\n",
       "60         1       20.742844   0.814292  0.813898    0.810349\n",
       "           2       21.556610   0.379375  0.381190    0.378412\n",
       "           3       21.468966   0.641542  0.652456    0.643428\n",
       "           4       21.421669   0.865042  0.866014    0.861461\n",
       "           5       21.288592   0.816833  0.817292    0.812044\n",
       "70         1       20.846441   0.594694  0.594449    0.587023\n",
       "           2       21.131107   0.778031  0.766573    0.768796\n",
       "           3       20.579022   0.448324  0.444289    0.439998\n",
       "           4       20.767252   0.508825  0.508087    0.503056\n",
       "           5       20.502395   0.434780  0.428315    0.431581\n",
       "80         1       19.752287   0.184167  0.183307    0.180312\n",
       "           2       19.717808   0.258417  0.261282    0.258911\n",
       "           3       20.207478   0.105667  0.113419    0.113813\n",
       "           4       21.309026   0.322833  0.325978    0.323997\n",
       "           5       19.911584   0.445250  0.446685    0.441395\n",
       "90         1       17.469472   0.525266  0.519369    0.515898\n",
       "           2       18.320393   0.113863  0.104433    0.105541\n",
       "           3       18.338756   0.106715  0.114617    0.114318\n",
       "           4       18.129450   0.206283  0.214557    0.215642\n",
       "           5       17.954962   0.166390  0.170627    0.165683\n",
       "99         1        6.548265   0.153509  0.159545    0.157805\n",
       "           2        6.961410   0.170504  0.166733    0.165726\n",
       "           3        6.759411   0.118969  0.126098    0.123574\n",
       "           4        6.917278   0.112939  0.095847    0.098085\n",
       "           5        6.966128   0.075110  0.091154    0.093361"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2de27d-b92c-465a-aac3-911cc0839b89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
