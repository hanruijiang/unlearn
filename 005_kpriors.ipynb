{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b703d374-8ed7-4fbc-9241-543d4c3883c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "\n",
    "import torch\n",
    "\n",
    "import copy\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13c94faf-8d6c-44aa-9618-7192902b1933",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_original_dataset, load_deleted_dataset\n",
    "from models import CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19d524cc-f9e7-4806-af84-a4429d0e47e0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = 'Datasets/Features/'\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5\n",
    "PERCENTAGES = [1, 10, 20, 30, 40, 50, 60, 70, 80, 90, 99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48a614e7-46f8-451b-aa48-9624b48efe77",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs('results/kpriors', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d647010e-c768-42fb-adf9-af6d0a83e0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = True\n",
    "\n",
    "# https://github.com/team-approx-bayes/kpriors/blob/main/main.py#L71\n",
    "learning_rate = 0.005\n",
    "\n",
    "# https://github.com/team-approx-bayes/kpriors/blob/main/main.py#L20\n",
    "adaptation_task = 'remove_data'\n",
    "\n",
    "# https://github.com/team-approx-bayes/kpriors/blob/main/main.py#L83\n",
    "prior_prec = 5\n",
    "\n",
    "# https://github.com/team-approx-bayes/kpriors/blob/main/main.py#L31\n",
    "adaptation_methods = ['Replay','K-priors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2108aae4-de7a-4176-bc15-369c134f91e8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath('./libraries/kpriors/'))\n",
    "\n",
    "# https://github.com/team-approx-bayes/kpriors/blob/main/main.py#L114\n",
    "from adamreg import AdamReg\n",
    "import utils\n",
    "import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00892bc0-072b-4e49-b728-feb52c23d270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/team-approx-bayes/kpriors/blob/main/models.py#L75\n",
    "\n",
    "# Return all parameters as a vector\n",
    "def return_parameters(model):\n",
    "    num_params = sum([np.prod(p.size()) for p in model.parameters()])\n",
    "    means = torch.zeros(num_params)\n",
    "\n",
    "    start_ind = 0\n",
    "    for p in model.parameters():\n",
    "        num = np.prod(p.size())\n",
    "        means[start_ind:start_ind+num] = p.data.reshape(-1)\n",
    "        start_ind += num\n",
    "\n",
    "    return means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c22c42d-40af-4ef6-b18c-103c2678403b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fit(base_model, save_dir, train_set, test_set, forget_set):\n",
    "    \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # https://github.com/team-approx-bayes/kpriors/blob/main/train.py#L10\n",
    "    error = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    # prepare model\n",
    "    \n",
    "    # https://github.com/team-approx-bayes/kpriors/blob/main/main.py#L159\n",
    "    model = copy.deepcopy(base_model)\n",
    "    optimiser = AdamReg(model, lr=learning_rate, weight_decay=prior_prec)\n",
    "    optimiser.previous_weights = return_parameters(base_model)\n",
    "\n",
    "    def select_memory_points(base_train_data, additional_memory_data):\n",
    "        \n",
    "        # Select points\n",
    "        # https://github.com/team-approx-bayes/kpriors/blob/main/main.py#L144\n",
    "        memory_points = {}\n",
    "        # https://github.com/team-approx-bayes/kpriors/blob/main/utils.py#L63\n",
    "        # https://github.com/team-approx-bayes/kpriors/blob/main/utils.py#L72\n",
    "        memory_points['inputs'] = torch.cat((base_train_data[0], additional_memory_data[0]))\n",
    "        memory_points['true_labels'] = torch.cat((base_train_data[1], additional_memory_data[1]))\n",
    "        if use_cuda:\n",
    "            memory_points['inputs'] = memory_points['inputs'].cuda()\n",
    "        # https://github.com/team-approx-bayes/kpriors/blob/main/utils.py#L79\n",
    "        memory_points['soft_labels'] = torch.softmax(base_model.forward(memory_points['inputs']), dim=-1)\n",
    "        \n",
    "        \n",
    "        # Soft labels in K-priors, hard (true) labels in Replay\n",
    "        if adaptation_method == \"K-priors\":\n",
    "            memory_points['labels'] = memory_points['soft_labels']\n",
    "        elif adaptation_method == \"Replay\":\n",
    "            memory_points['labels'] = torch.nn.functional.one_hot(memory_points['true_labels'], num_classes=10)\n",
    "        \n",
    "        # Store past memory labels\n",
    "        # https://github.com/team-approx-bayes/kpriors/blob/main/main.py#L230\n",
    "        optimiser.memory_labels = memory_points['labels']\n",
    "\n",
    "        return memory_points\n",
    "\n",
    "    \n",
    "    # https://github.com/team-approx-bayes/kpriors/blob/main/main.py#L218\n",
    "    optimiser.prior_prec_old = prior_prec\n",
    "    remove_data_bool = True\n",
    "\n",
    "    # https://github.com/team-approx-bayes/kpriors/blob/main/main.py#L49\n",
    "    train_batch_size = int(np.ceil(BATCH_SIZE * len(train_set) / (len(train_set) + len(forget_set))))\n",
    "    forget_batch_size = int(np.ceil(BATCH_SIZE * len(forget_set) / (len(train_set) + len(forget_set))))\n",
    "\n",
    "    num_steps = min(len(train_set) // train_batch_size, len(forget_set) // forget_batch_size)\n",
    "\n",
    "    train_x, train_y = train_set.tensors[0], train_set.tensors[1]\n",
    "    forget_x, forget_y = forget_set.tensors[0], forget_set.tensors[1]\n",
    "    \n",
    "    train_times = list()\n",
    "    \n",
    "    train_accs, test_accs, forget_accs = list(), list(), list()\n",
    "    \n",
    "    for epoch in range(EPOCHS):    \n",
    "        \n",
    "        # train\n",
    "        \n",
    "        train_time = 0\n",
    "        \n",
    "        start_time = time.time()\n",
    "\n",
    "        model.train()\n",
    "        for i in range(num_steps):\n",
    "            \n",
    "            # If remove_data task, then store the removed points too, for both K-priors and Replay\n",
    "            \n",
    "            # https://github.com/team-approx-bayes/kpriors/blob/main/main.py#L140\n",
    "            # https://github.com/team-approx-bayes/kpriors/blob/main/main.py#L134\n",
    "            base_train_data = (train_x[train_batch_size*i:train_batch_size*(i+1)], train_y[train_batch_size*i:train_batch_size*(i+1)])\n",
    "            additional_memory_data = (forget_x[forget_batch_size*i:forget_batch_size*(i+1)], forget_y[forget_batch_size*i:forget_batch_size*(i+1)])\n",
    "            \n",
    "            # Load data for adaptation task\n",
    "            # https://github.com/team-approx-bayes/kpriors/blob/main/main.py#L148\n",
    "            # https://github.com/team-approx-bayes/kpriors/blob/main/data_generators.py#L67\n",
    "            adapt_train_data = additional_memory_data\n",
    "    \n",
    "            memory_points = select_memory_points(base_train_data, additional_memory_data)\n",
    "                \n",
    "            # Train model\n",
    "            # https://github.com/team-approx-bayes/kpriors/blob/main/main.py#L235\n",
    "            \n",
    "            train.train_model(\n",
    "                model, optimiser, adapt_train_data, \n",
    "                num_epochs=1, # one epoch\n",
    "                memory_data=memory_points,\n",
    "                adaptation_method=adaptation_method, \n",
    "                remove_data_bool=remove_data_bool, \n",
    "                use_cuda=use_cuda\n",
    "            )\n",
    "        \n",
    "        train_time += time.time() - start_time\n",
    "            \n",
    "        train_times.append(train_time)\n",
    "        \n",
    "        # test\n",
    "            \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            #\n",
    "            \n",
    "            x, y = train_set.tensors\n",
    "            \n",
    "            accs = list()\n",
    "            \n",
    "            for i in range(0, x.shape[0], BATCH_SIZE):\n",
    "            \n",
    "                output = model(x[i:i+BATCH_SIZE].cuda())\n",
    "\n",
    "                predicted = torch.argmax(output.data, dim=-1)\n",
    "                accs.append((predicted == y[i:i+BATCH_SIZE].cuda()).float().mean().detach().cpu().numpy())\n",
    "            \n",
    "            train_accs.append(np.mean(accs))\n",
    "            \n",
    "            #\n",
    "            \n",
    "            x, y = test_set.tensors\n",
    "            \n",
    "            accs = list()\n",
    "            \n",
    "            for i in range(0, x.shape[0], BATCH_SIZE):\n",
    "            \n",
    "                output = model(x[i:i+BATCH_SIZE].cuda())\n",
    "\n",
    "                predicted = torch.argmax(output.data, dim=-1)\n",
    "                accs.append((predicted == y[i:i+BATCH_SIZE].cuda()).float().mean().detach().cpu().numpy())\n",
    "            \n",
    "            test_accs.append(np.mean(accs))\n",
    "            \n",
    "            #\n",
    "\n",
    "            x, y = forget_set.tensors\n",
    "\n",
    "            for i in range(0, x.shape[0], BATCH_SIZE):\n",
    "\n",
    "                output = model(x[i:i+BATCH_SIZE].cuda())\n",
    "\n",
    "                predicted = torch.argmax(output.data, dim=-1)\n",
    "                accs.append((predicted == y[i:i+BATCH_SIZE].cuda()).float().mean().detach().cpu().numpy())\n",
    "\n",
    "            forget_accs.append(np.mean(accs))\n",
    "        \n",
    "        # save\n",
    "        torch.save(model.state_dict(), os.path.join(save_dir, f'{(epoch+1):03d}.pt'))\n",
    "\n",
    "    return train_times, train_accs, test_accs, forget_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ad852eb-cb3d-4f0c-99ec-606bc20cfd4a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7fb989d77074c12be56b18884f6b849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "039655c8fe35497997e96c71c3a7957d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for adaptation_method in adaptation_methods:\n",
    "\n",
    "    results = list()\n",
    "    \n",
    "    for percentage in tqdm(PERCENTAGES):\n",
    "        \n",
    "        model = CNN().cuda()\n",
    "        \n",
    "        model.load_state_dict(torch.load('./weights/original/005.pt'))\n",
    "        \n",
    "        train_set, test_set, forget_set = load_deleted_dataset(DATA_DIR, percentage)\n",
    "        \n",
    "        train_times, train_accs, test_accs, forget_accs = fit(model, f'weights/kpriors/{adaptation_method}/{percentage}', train_set, test_set, forget_set)\n",
    "        \n",
    "        df = pd.DataFrame(zip(train_times, train_accs, test_accs, forget_accs), columns=['train_time', 'train_acc', 'test_acc', 'forget_acc'])\n",
    "        df['epoch'] = range(1, EPOCHS+1)\n",
    "        df['percentage'] = percentage\n",
    "        \n",
    "        results.append(df)\n",
    "\n",
    "    results = pd.concat(results).set_index(['percentage', 'epoch'])\n",
    "    \n",
    "    results.to_csv(f'results/kpriors/{adaptation_method}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7d0a870-9f24-4a6c-acc8-e75b640da0c4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>train_time</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>test_acc</th>\n",
       "      <th>forget_acc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>percentage</th>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th>1</th>\n",
       "      <td>6.246719</td>\n",
       "      <td>0.974051</td>\n",
       "      <td>0.972843</td>\n",
       "      <td>0.973048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.432508</td>\n",
       "      <td>0.976794</td>\n",
       "      <td>0.979034</td>\n",
       "      <td>0.979198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.457139</td>\n",
       "      <td>0.974573</td>\n",
       "      <td>0.974840</td>\n",
       "      <td>0.974743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.407178</td>\n",
       "      <td>0.963786</td>\n",
       "      <td>0.961462</td>\n",
       "      <td>0.962067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.530234</td>\n",
       "      <td>0.972099</td>\n",
       "      <td>0.972943</td>\n",
       "      <td>0.972923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">10</th>\n",
       "      <th>1</th>\n",
       "      <td>16.206944</td>\n",
       "      <td>0.971564</td>\n",
       "      <td>0.971246</td>\n",
       "      <td>0.972118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16.340193</td>\n",
       "      <td>0.947275</td>\n",
       "      <td>0.953175</td>\n",
       "      <td>0.951722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.465451</td>\n",
       "      <td>0.958198</td>\n",
       "      <td>0.962660</td>\n",
       "      <td>0.962325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.547714</td>\n",
       "      <td>0.959938</td>\n",
       "      <td>0.961761</td>\n",
       "      <td>0.962450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>16.548919</td>\n",
       "      <td>0.972508</td>\n",
       "      <td>0.974042</td>\n",
       "      <td>0.974613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">20</th>\n",
       "      <th>1</th>\n",
       "      <td>18.558873</td>\n",
       "      <td>0.967458</td>\n",
       "      <td>0.967851</td>\n",
       "      <td>0.964435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.057752</td>\n",
       "      <td>0.953354</td>\n",
       "      <td>0.953774</td>\n",
       "      <td>0.952444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19.304313</td>\n",
       "      <td>0.947625</td>\n",
       "      <td>0.947085</td>\n",
       "      <td>0.945176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19.363307</td>\n",
       "      <td>0.964354</td>\n",
       "      <td>0.965056</td>\n",
       "      <td>0.962436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>19.251297</td>\n",
       "      <td>0.963646</td>\n",
       "      <td>0.967153</td>\n",
       "      <td>0.964208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">30</th>\n",
       "      <th>1</th>\n",
       "      <td>20.592468</td>\n",
       "      <td>0.633878</td>\n",
       "      <td>0.637081</td>\n",
       "      <td>0.623430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20.892775</td>\n",
       "      <td>0.701495</td>\n",
       "      <td>0.711961</td>\n",
       "      <td>0.707156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20.557113</td>\n",
       "      <td>0.911724</td>\n",
       "      <td>0.917831</td>\n",
       "      <td>0.913313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.637675</td>\n",
       "      <td>0.929265</td>\n",
       "      <td>0.930511</td>\n",
       "      <td>0.928403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20.394286</td>\n",
       "      <td>0.956826</td>\n",
       "      <td>0.956669</td>\n",
       "      <td>0.956193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">40</th>\n",
       "      <th>1</th>\n",
       "      <td>21.388638</td>\n",
       "      <td>0.914139</td>\n",
       "      <td>0.914736</td>\n",
       "      <td>0.915275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21.023240</td>\n",
       "      <td>0.931611</td>\n",
       "      <td>0.931010</td>\n",
       "      <td>0.929004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20.577450</td>\n",
       "      <td>0.708250</td>\n",
       "      <td>0.711861</td>\n",
       "      <td>0.711636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.893219</td>\n",
       "      <td>0.924056</td>\n",
       "      <td>0.923522</td>\n",
       "      <td>0.922713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>21.067666</td>\n",
       "      <td>0.749028</td>\n",
       "      <td>0.750499</td>\n",
       "      <td>0.749147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">50</th>\n",
       "      <th>1</th>\n",
       "      <td>21.679158</td>\n",
       "      <td>0.917011</td>\n",
       "      <td>0.920427</td>\n",
       "      <td>0.915568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21.708622</td>\n",
       "      <td>0.792344</td>\n",
       "      <td>0.790335</td>\n",
       "      <td>0.793865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21.716053</td>\n",
       "      <td>0.508162</td>\n",
       "      <td>0.512780</td>\n",
       "      <td>0.508268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21.696955</td>\n",
       "      <td>0.829091</td>\n",
       "      <td>0.839257</td>\n",
       "      <td>0.827638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>21.867433</td>\n",
       "      <td>0.446995</td>\n",
       "      <td>0.461861</td>\n",
       "      <td>0.454761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">60</th>\n",
       "      <th>1</th>\n",
       "      <td>20.983034</td>\n",
       "      <td>0.161208</td>\n",
       "      <td>0.163938</td>\n",
       "      <td>0.159771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21.960438</td>\n",
       "      <td>0.313958</td>\n",
       "      <td>0.320288</td>\n",
       "      <td>0.317650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21.620485</td>\n",
       "      <td>0.263208</td>\n",
       "      <td>0.258886</td>\n",
       "      <td>0.259714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.987870</td>\n",
       "      <td>0.742000</td>\n",
       "      <td>0.746506</td>\n",
       "      <td>0.736852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>22.452072</td>\n",
       "      <td>0.469083</td>\n",
       "      <td>0.489617</td>\n",
       "      <td>0.478790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">70</th>\n",
       "      <th>1</th>\n",
       "      <td>20.539524</td>\n",
       "      <td>0.397980</td>\n",
       "      <td>0.385184</td>\n",
       "      <td>0.382937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21.192062</td>\n",
       "      <td>0.684447</td>\n",
       "      <td>0.682708</td>\n",
       "      <td>0.675027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20.781398</td>\n",
       "      <td>0.372058</td>\n",
       "      <td>0.360224</td>\n",
       "      <td>0.356896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21.042561</td>\n",
       "      <td>0.655029</td>\n",
       "      <td>0.646965</td>\n",
       "      <td>0.647582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20.936912</td>\n",
       "      <td>0.370060</td>\n",
       "      <td>0.374700</td>\n",
       "      <td>0.371118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">80</th>\n",
       "      <th>1</th>\n",
       "      <td>19.810648</td>\n",
       "      <td>0.424833</td>\n",
       "      <td>0.431609</td>\n",
       "      <td>0.427313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.841686</td>\n",
       "      <td>0.668167</td>\n",
       "      <td>0.667532</td>\n",
       "      <td>0.655923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19.934380</td>\n",
       "      <td>0.309167</td>\n",
       "      <td>0.312400</td>\n",
       "      <td>0.310569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.188042</td>\n",
       "      <td>0.482750</td>\n",
       "      <td>0.476338</td>\n",
       "      <td>0.473697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>19.978985</td>\n",
       "      <td>0.205167</td>\n",
       "      <td>0.203874</td>\n",
       "      <td>0.201772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">90</th>\n",
       "      <th>1</th>\n",
       "      <td>18.138252</td>\n",
       "      <td>0.468085</td>\n",
       "      <td>0.449281</td>\n",
       "      <td>0.447573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.247383</td>\n",
       "      <td>0.124834</td>\n",
       "      <td>0.119708</td>\n",
       "      <td>0.120003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17.698911</td>\n",
       "      <td>0.345578</td>\n",
       "      <td>0.326777</td>\n",
       "      <td>0.332662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18.578517</td>\n",
       "      <td>0.334940</td>\n",
       "      <td>0.336062</td>\n",
       "      <td>0.331209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>18.051911</td>\n",
       "      <td>0.103890</td>\n",
       "      <td>0.102436</td>\n",
       "      <td>0.102558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">99</th>\n",
       "      <th>1</th>\n",
       "      <td>6.644415</td>\n",
       "      <td>0.098136</td>\n",
       "      <td>0.118710</td>\n",
       "      <td>0.117828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.956798</td>\n",
       "      <td>0.080592</td>\n",
       "      <td>0.084465</td>\n",
       "      <td>0.082661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.200098</td>\n",
       "      <td>0.077851</td>\n",
       "      <td>0.083766</td>\n",
       "      <td>0.082935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.026857</td>\n",
       "      <td>0.110197</td>\n",
       "      <td>0.104233</td>\n",
       "      <td>0.100922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7.380495</td>\n",
       "      <td>0.098684</td>\n",
       "      <td>0.094349</td>\n",
       "      <td>0.096486</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  train_time  train_acc  test_acc  forget_acc\n",
       "percentage epoch                                             \n",
       "1          1        6.246719   0.974051  0.972843    0.973048\n",
       "           2        6.432508   0.976794  0.979034    0.979198\n",
       "           3        6.457139   0.974573  0.974840    0.974743\n",
       "           4        6.407178   0.963786  0.961462    0.962067\n",
       "           5        6.530234   0.972099  0.972943    0.972923\n",
       "10         1       16.206944   0.971564  0.971246    0.972118\n",
       "           2       16.340193   0.947275  0.953175    0.951722\n",
       "           3       16.465451   0.958198  0.962660    0.962325\n",
       "           4       16.547714   0.959938  0.961761    0.962450\n",
       "           5       16.548919   0.972508  0.974042    0.974613\n",
       "20         1       18.558873   0.967458  0.967851    0.964435\n",
       "           2       19.057752   0.953354  0.953774    0.952444\n",
       "           3       19.304313   0.947625  0.947085    0.945176\n",
       "           4       19.363307   0.964354  0.965056    0.962436\n",
       "           5       19.251297   0.963646  0.967153    0.964208\n",
       "30         1       20.592468   0.633878  0.637081    0.623430\n",
       "           2       20.892775   0.701495  0.711961    0.707156\n",
       "           3       20.557113   0.911724  0.917831    0.913313\n",
       "           4       20.637675   0.929265  0.930511    0.928403\n",
       "           5       20.394286   0.956826  0.956669    0.956193\n",
       "40         1       21.388638   0.914139  0.914736    0.915275\n",
       "           2       21.023240   0.931611  0.931010    0.929004\n",
       "           3       20.577450   0.708250  0.711861    0.711636\n",
       "           4       20.893219   0.924056  0.923522    0.922713\n",
       "           5       21.067666   0.749028  0.750499    0.749147\n",
       "50         1       21.679158   0.917011  0.920427    0.915568\n",
       "           2       21.708622   0.792344  0.790335    0.793865\n",
       "           3       21.716053   0.508162  0.512780    0.508268\n",
       "           4       21.696955   0.829091  0.839257    0.827638\n",
       "           5       21.867433   0.446995  0.461861    0.454761\n",
       "60         1       20.983034   0.161208  0.163938    0.159771\n",
       "           2       21.960438   0.313958  0.320288    0.317650\n",
       "           3       21.620485   0.263208  0.258886    0.259714\n",
       "           4       20.987870   0.742000  0.746506    0.736852\n",
       "           5       22.452072   0.469083  0.489617    0.478790\n",
       "70         1       20.539524   0.397980  0.385184    0.382937\n",
       "           2       21.192062   0.684447  0.682708    0.675027\n",
       "           3       20.781398   0.372058  0.360224    0.356896\n",
       "           4       21.042561   0.655029  0.646965    0.647582\n",
       "           5       20.936912   0.370060  0.374700    0.371118\n",
       "80         1       19.810648   0.424833  0.431609    0.427313\n",
       "           2       19.841686   0.668167  0.667532    0.655923\n",
       "           3       19.934380   0.309167  0.312400    0.310569\n",
       "           4       20.188042   0.482750  0.476338    0.473697\n",
       "           5       19.978985   0.205167  0.203874    0.201772\n",
       "90         1       18.138252   0.468085  0.449281    0.447573\n",
       "           2       18.247383   0.124834  0.119708    0.120003\n",
       "           3       17.698911   0.345578  0.326777    0.332662\n",
       "           4       18.578517   0.334940  0.336062    0.331209\n",
       "           5       18.051911   0.103890  0.102436    0.102558\n",
       "99         1        6.644415   0.098136  0.118710    0.117828\n",
       "           2        6.956798   0.080592  0.084465    0.082661\n",
       "           3        7.200098   0.077851  0.083766    0.082935\n",
       "           4        7.026857   0.110197  0.104233    0.100922\n",
       "           5        7.380495   0.098684  0.094349    0.096486"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2de27d-b92c-465a-aac3-911cc0839b89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
