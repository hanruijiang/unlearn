{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c5517c9-eab1-483c-80a2-e332fc57d686",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "import copy\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13c94faf-8d6c-44aa-9618-7192902b1933",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_original_dataset, load_deleted_dataset\n",
    "from models import CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19d524cc-f9e7-4806-af84-a4429d0e47e0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = 'Datasets/Features/'\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5\n",
    "PERCENTAGES = [1, 10, 20, 30, 40, 50, 60, 70, 80, 90, 99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48a614e7-46f8-451b-aa48-9624b48efe77",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs('results/unrolling-sgd', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabe569d-1a5f-4a7f-b90c-5144bafa7a29",
   "metadata": {},
   "source": [
    "# finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24e1d64a-eb5d-4980-ac01-73000360b291",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fit(model, save_dir, train_set, test_set, forget_set):\n",
    "    \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # prepare model\n",
    "    \n",
    "    # https://github.com/cleverhans-lab/unrolling-sgd/blob/main/Unlearning/finetune.py#L94\n",
    "    M = copy.deepcopy(model)\n",
    "    \n",
    "    # https://github.com/cleverhans-lab/unrolling-sgd/blob/main/Unlearning/finetune.py#L150\n",
    "    M_unlearned = copy.deepcopy(model)\n",
    "        \n",
    "    M_unlearned.train()\n",
    "    \n",
    "    # https://github.com/cleverhans-lab/unrolling-sgd/blob/main/Unlearning/finetune.py#L131\n",
    "    error = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    forget_loader = torch.utils.data.DataLoader(forget_set, batch_size = BATCH_SIZE, shuffle = True, drop_last=True)\n",
    "    \n",
    "    train_times = list()\n",
    "    train_accs, test_accs, forget_accs = list(), list(), list()\n",
    "    \n",
    "    for epoch in range(EPOCHS):    \n",
    "        \n",
    "        # train\n",
    "        \n",
    "        train_time = 0\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for x, y in forget_loader:\n",
    "\n",
    "            output = M_unlearned(x.cuda())\n",
    "            y = y.cuda()\n",
    "            loss = error(output, y)\n",
    "            \n",
    "            # https://github.com/cleverhans-lab/unrolling-sgd/blob/main/Unlearning/finetune.py#L163\n",
    "            loss.backward(retain_graph=True)\n",
    "            grads = torch.autograd.grad(loss, [param for param in M_unlearned.parameters()], create_graph = True)\n",
    "            \n",
    "            old_params = {}\n",
    "            for i, (name, params) in enumerate(M.named_parameters()):\n",
    "                    old_params[name] = params.clone()\n",
    "                    old_params[name] += LR * grads[i]\n",
    "            for name, params in M_unlearned.named_parameters():\n",
    "                    params.data.copy_(old_params[name])\n",
    "            \n",
    "            train_time += time.time() - start_time\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "        train_times.append(train_time)\n",
    "        \n",
    "        # test\n",
    "            \n",
    "        M_unlearned.eval()\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            #\n",
    "            \n",
    "            x, y = train_set.tensors\n",
    "            \n",
    "            accs = list()\n",
    "            \n",
    "            for i in range(0, x.shape[0], BATCH_SIZE):\n",
    "            \n",
    "                output = M_unlearned(x[i:i+BATCH_SIZE].cuda())\n",
    "\n",
    "                predicted = torch.argmax(output.data, dim=-1)\n",
    "                accs.append((predicted == y[i:i+BATCH_SIZE].cuda()).float().mean().detach().cpu().numpy())\n",
    "            \n",
    "            train_accs.append(np.mean(accs))\n",
    "            \n",
    "            #\n",
    "            \n",
    "            x, y = test_set.tensors\n",
    "            \n",
    "            accs = list()\n",
    "            \n",
    "            for i in range(0, x.shape[0], BATCH_SIZE):\n",
    "            \n",
    "                output = M_unlearned(x[i:i+BATCH_SIZE].cuda())\n",
    "\n",
    "                predicted = torch.argmax(output.data, dim=-1)\n",
    "                accs.append((predicted == y[i:i+BATCH_SIZE].cuda()).float().mean().detach().cpu().numpy())\n",
    "            \n",
    "            test_accs.append(np.mean(accs))\n",
    "            \n",
    "            #\n",
    "\n",
    "            x, y = forget_set.tensors\n",
    "            \n",
    "            accs = list()\n",
    "\n",
    "            for i in range(0, x.shape[0], BATCH_SIZE):\n",
    "\n",
    "                output = M_unlearned(x[i:i+BATCH_SIZE].cuda())\n",
    "\n",
    "                predicted = torch.argmax(output.data, dim=-1)\n",
    "                accs.append((predicted == y[i:i+BATCH_SIZE].cuda()).float().mean().detach().cpu().numpy())\n",
    "\n",
    "            forget_accs.append(np.mean(accs))\n",
    "        \n",
    "        # save\n",
    "        torch.save(M_unlearned.state_dict(), os.path.join(save_dir, f'{(epoch+1):03d}.pt'))\n",
    "\n",
    "    return train_times, train_accs, test_accs, forget_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f92ac6b-f76c-4dde-ba0a-915d5ba15b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/cleverhans-lab/unrolling-sgd/blob/main/Unlearning/finetune.py#L49\n",
    "LR = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a81ffe7-8712-4760-b4bf-072e4d26535a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "572eabc198314298b3fb640b9a6f73b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = list()\n",
    "\n",
    "for percentage in tqdm(PERCENTAGES):\n",
    "    \n",
    "    model = CNN().cuda()\n",
    "    \n",
    "    model.load_state_dict(torch.load('./weights/original/005.pt'))\n",
    "    \n",
    "    train_set, test_set, forget_set = load_deleted_dataset(DATA_DIR, percentage)\n",
    "    \n",
    "    train_times, train_accs, test_accs, forget_accs = fit(model, f'weights/unrolling-sgd/finetune/{percentage}', train_set, test_set, forget_set)\n",
    "    \n",
    "    df = pd.DataFrame(zip(train_times, train_accs, test_accs, forget_accs), columns=['train_time', 'train_acc', 'test_acc', 'forget_acc'])\n",
    "    df['epoch'] = range(1, EPOCHS+1)\n",
    "    df['percentage'] = percentage\n",
    "    \n",
    "    results.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f09bcabd-676b-4172-ad65-1ba2117bd35c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>train_time</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>test_acc</th>\n",
       "      <th>forget_acc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>percentage</th>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th>1</th>\n",
       "      <td>0.861320</td>\n",
       "      <td>0.098697</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.097039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.149785</td>\n",
       "      <td>0.098697</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.097039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.129887</td>\n",
       "      <td>0.098697</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.097039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.141159</td>\n",
       "      <td>0.098697</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.097039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.121428</td>\n",
       "      <td>0.098697</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.097039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">10</th>\n",
       "      <th>1</th>\n",
       "      <td>1.649805</td>\n",
       "      <td>0.098711</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.710710</td>\n",
       "      <td>0.098711</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.690747</td>\n",
       "      <td>0.098711</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.691322</td>\n",
       "      <td>0.098711</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.687659</td>\n",
       "      <td>0.098711</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">20</th>\n",
       "      <th>1</th>\n",
       "      <td>3.563996</td>\n",
       "      <td>0.098271</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.100500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.589719</td>\n",
       "      <td>0.098271</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.100500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.581025</td>\n",
       "      <td>0.098271</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.100500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.538590</td>\n",
       "      <td>0.098271</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.100500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.544239</td>\n",
       "      <td>0.098271</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.100500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">30</th>\n",
       "      <th>1</th>\n",
       "      <td>5.725345</td>\n",
       "      <td>0.098558</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.099023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.367122</td>\n",
       "      <td>0.098558</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.099023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.377095</td>\n",
       "      <td>0.098558</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.099023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.430189</td>\n",
       "      <td>0.098558</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.099023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.424914</td>\n",
       "      <td>0.098558</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.099023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">40</th>\n",
       "      <th>1</th>\n",
       "      <td>8.000167</td>\n",
       "      <td>0.098389</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.099208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.510090</td>\n",
       "      <td>0.098389</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.099208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.129795</td>\n",
       "      <td>0.098389</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.099208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.136111</td>\n",
       "      <td>0.098389</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.099208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7.110609</td>\n",
       "      <td>0.098389</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.099208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">50</th>\n",
       "      <th>1</th>\n",
       "      <td>10.072822</td>\n",
       "      <td>0.098714</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.602136</td>\n",
       "      <td>0.098714</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.110337</td>\n",
       "      <td>0.098714</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.109370</td>\n",
       "      <td>0.098714</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9.046206</td>\n",
       "      <td>0.098714</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">60</th>\n",
       "      <th>1</th>\n",
       "      <td>11.643538</td>\n",
       "      <td>0.099542</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.602450</td>\n",
       "      <td>0.099542</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.747267</td>\n",
       "      <td>0.099542</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.938334</td>\n",
       "      <td>0.099542</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10.460525</td>\n",
       "      <td>0.099542</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">70</th>\n",
       "      <th>1</th>\n",
       "      <td>13.330676</td>\n",
       "      <td>0.098857</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.482202</td>\n",
       "      <td>0.098857</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12.519452</td>\n",
       "      <td>0.098857</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.007364</td>\n",
       "      <td>0.098857</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12.460160</td>\n",
       "      <td>0.098857</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">80</th>\n",
       "      <th>1</th>\n",
       "      <td>15.516491</td>\n",
       "      <td>0.099500</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14.571281</td>\n",
       "      <td>0.099500</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.648647</td>\n",
       "      <td>0.099500</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.046360</td>\n",
       "      <td>0.099500</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14.730189</td>\n",
       "      <td>0.099500</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">90</th>\n",
       "      <th>1</th>\n",
       "      <td>18.407147</td>\n",
       "      <td>0.097739</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17.392190</td>\n",
       "      <td>0.097739</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17.713189</td>\n",
       "      <td>0.097739</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.665290</td>\n",
       "      <td>0.097739</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>16.683773</td>\n",
       "      <td>0.097739</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">99</th>\n",
       "      <th>1</th>\n",
       "      <td>20.123055</td>\n",
       "      <td>0.095943</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.979310</td>\n",
       "      <td>0.095943</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18.767822</td>\n",
       "      <td>0.095943</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18.995738</td>\n",
       "      <td>0.095943</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>18.159758</td>\n",
       "      <td>0.095943</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098697</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  train_time  train_acc  test_acc  forget_acc\n",
       "percentage epoch                                             \n",
       "1          1        0.861320   0.098697  0.097943    0.097039\n",
       "           2        0.149785   0.098697  0.097943    0.097039\n",
       "           3        0.129887   0.098697  0.097943    0.097039\n",
       "           4        0.141159   0.098697  0.097943    0.097039\n",
       "           5        0.121428   0.098697  0.097943    0.097039\n",
       "10         1        1.649805   0.098711  0.097943    0.098404\n",
       "           2        1.710710   0.098711  0.097943    0.098404\n",
       "           3        1.690747   0.098711  0.097943    0.098404\n",
       "           4        1.691322   0.098711  0.097943    0.098404\n",
       "           5        1.687659   0.098711  0.097943    0.098404\n",
       "20         1        3.563996   0.098271  0.097943    0.100500\n",
       "           2        3.589719   0.098271  0.097943    0.100500\n",
       "           3        3.581025   0.098271  0.097943    0.100500\n",
       "           4        3.538590   0.098271  0.097943    0.100500\n",
       "           5        3.544239   0.098271  0.097943    0.100500\n",
       "30         1        5.725345   0.098558  0.097943    0.099023\n",
       "           2        5.367122   0.098558  0.097943    0.099023\n",
       "           3        5.377095   0.098558  0.097943    0.099023\n",
       "           4        5.430189   0.098558  0.097943    0.099023\n",
       "           5        5.424914   0.098558  0.097943    0.099023\n",
       "40         1        8.000167   0.098389  0.097943    0.099208\n",
       "           2        7.510090   0.098389  0.097943    0.099208\n",
       "           3        7.129795   0.098389  0.097943    0.099208\n",
       "           4        7.136111   0.098389  0.097943    0.099208\n",
       "           5        7.110609   0.098389  0.097943    0.099208\n",
       "50         1       10.072822   0.098714  0.097943    0.098714\n",
       "           2        9.602136   0.098714  0.097943    0.098714\n",
       "           3        9.110337   0.098714  0.097943    0.098714\n",
       "           4        9.109370   0.098714  0.097943    0.098714\n",
       "           5        9.046206   0.098714  0.097943    0.098714\n",
       "60         1       11.643538   0.099542  0.097943    0.098167\n",
       "           2       10.602450   0.099542  0.097943    0.098167\n",
       "           3       10.747267   0.099542  0.097943    0.098167\n",
       "           4       10.938334   0.099542  0.097943    0.098167\n",
       "           5       10.460525   0.099542  0.097943    0.098167\n",
       "70         1       13.330676   0.098857  0.097943    0.098677\n",
       "           2       12.482202   0.098857  0.097943    0.098677\n",
       "           3       12.519452   0.098857  0.097943    0.098677\n",
       "           4       13.007364   0.098857  0.097943    0.098677\n",
       "           5       12.460160   0.098857  0.097943    0.098677\n",
       "80         1       15.516491   0.099500  0.097943    0.098521\n",
       "           2       14.571281   0.099500  0.097943    0.098521\n",
       "           3       14.648647   0.099500  0.097943    0.098521\n",
       "           4       16.046360   0.099500  0.097943    0.098521\n",
       "           5       14.730189   0.099500  0.097943    0.098521\n",
       "90         1       18.407147   0.097739  0.097943    0.098804\n",
       "           2       17.392190   0.097739  0.097943    0.098804\n",
       "           3       17.713189   0.097739  0.097943    0.098804\n",
       "           4       16.665290   0.097739  0.097943    0.098804\n",
       "           5       16.683773   0.097739  0.097943    0.098804\n",
       "99         1       20.123055   0.095943  0.097943    0.098697\n",
       "           2       18.979310   0.095943  0.097943    0.098697\n",
       "           3       18.767822   0.095943  0.097943    0.098697\n",
       "           4       18.995738   0.095943  0.097943    0.098697\n",
       "           5       18.159758   0.095943  0.097943    0.098697"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.concat(results).set_index(['percentage', 'epoch'])\n",
    "\n",
    "results.to_csv('results/unrolling-sgd/finetune.csv')\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1186c623-c225-4167-945e-9f9239d0f59a",
   "metadata": {},
   "source": [
    "# correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a47633c-9ee2-4e99-90f1-0c52b74f0016",
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_loss(x, y):\n",
    "    log_prob = -1.0 * F.log_softmax(x, 1)\n",
    "    loss = log_prob.gather(1, y.unsqueeze(1))\n",
    "    loss = loss.mean()\n",
    "    avg_std = torch.sum(torch.std(x, dim=1))/(len(x.view(-1)))\n",
    "    loss = loss + STD_REG*avg_std\n",
    "    return loss\n",
    "\n",
    "def my_cross_entropy(x, y):\n",
    "    log_prob = -1.0 * F.log_softmax(x, 1)\n",
    "    loss = log_prob.gather(1, y.unsqueeze(1))\n",
    "    loss = loss.mean()\n",
    "\n",
    "    #x is (N,C)\n",
    "    N,C = x.shape\n",
    "    p = F.softmax(x,1)\n",
    "    hessian_loss = torch.sum(p *(1-p),dim =1)\n",
    "    hessian_loss = hessian_loss.mean()\n",
    "\n",
    "    loss = loss + STD_REG * hessian_loss\n",
    "    return loss\n",
    "\n",
    "# https://github.com/cleverhans-lab/unrolling-sgd/blob/main/Unlearning/Correlation_unlearning.py#L130\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9368b9e-4fdf-47ee-bb07-dae3d7e6512f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, save_dir, train_set, test_set, forget_set):\n",
    "    \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # https://github.com/cleverhans-lab/unrolling-sgd/blob/main/Unlearning/Correlation_unlearning.py#L161\n",
    "\n",
    "    M = copy.deepcopy(model)\n",
    "\n",
    "    M.train()\n",
    "    \n",
    "    # https://github.com/cleverhans-lab/unrolling-sgd/blob/main/Unlearning/Correlation_unlearning.py#L172\n",
    "    optimizer = torch.optim.SGD(M.parameters(), lr=LR, weight_decay = L2)\n",
    "    \n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size = BATCH_SIZE, shuffle = True, drop_last=True)\n",
    "    \n",
    "    forget_loader = torch.utils.data.DataLoader(forget_set, batch_size = BATCH_SIZE, shuffle = True, drop_last=True)\n",
    "    \n",
    "    train_times = list()\n",
    "    train_accs, test_accs, forget_accs = list(), list(), list()\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        \n",
    "        # train\n",
    "        \n",
    "        train_time = 0\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # https://github.com/cleverhans-lab/unrolling-sgd/blob/main/Unlearning/Correlation_unlearning.py#L200\n",
    "        \n",
    "        # only update M:\n",
    "        \n",
    "        M.train()\n",
    "        \n",
    "        for x, y in forget_loader:\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs_M = M(x.cuda())\n",
    "            \n",
    "            targets = y.cuda()\n",
    "        \n",
    "            if LOSS_FUNC == 'hess':\n",
    "                loss_M = my_cross_entropy(outputs_M, targets)\n",
    "            if LOSS_FUNC =='std':\n",
    "                loss_M = std_loss(outputs_M, targets)\n",
    "            if LOSS_FUNC =='regular':\n",
    "                loss_M = loss_fn(outputs_M, targets)\n",
    "                \n",
    "            loss_M.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # now we also want to compute the gradient:\n",
    "        \n",
    "        grad_list = []\n",
    "        \n",
    "        for x, y in train_loader:\n",
    "\n",
    "            output_grad = M(x.cuda())\n",
    "            \n",
    "            label = y.cuda()\n",
    "            \n",
    "            if LOSS_FUNC == 'hess':\n",
    "                loss_grad = my_cross_entropy(output_grad, label)\n",
    "            if LOSS_FUNC =='std':\n",
    "                loss_grad = std_loss(output_grad, label)\n",
    "            if LOSS_FUNC =='regular':\n",
    "                loss_grad = loss_fn(output_grad, label)\n",
    "            \n",
    "            loss_grad.backward(retain_graph=True)\n",
    "            grads = torch.autograd.grad(loss_grad, [param for param in M.parameters()], create_graph = True)\n",
    "            # !!! modified !!! original code cause OOM\n",
    "            # grad_list.append(grads)\n",
    "            grad_list.append([i.detach().cpu() for i in grads])\n",
    "        \n",
    "        # https://github.com/cleverhans-lab/unrolling-sgd/blob/main/Unlearning/Correlation_unlearning.py#L227\n",
    "            \n",
    "        # update both M and M'\n",
    "        \n",
    "        for x, y in forget_loader:\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs_M = M(x.cuda())\n",
    "            \n",
    "            targets = y.cuda()\n",
    "        \n",
    "            if LOSS_FUNC == 'hess':\n",
    "                loss_M = my_cross_entropy(outputs_M, targets)\n",
    "            if LOSS_FUNC =='std':\n",
    "                loss_M = std_loss(outputs_M, targets)\n",
    "            if LOSS_FUNC =='regular':\n",
    "                loss_M = loss_fn(outputs_M, targets)\n",
    "                \n",
    "            loss_M.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        # Now, get M''_(N+t)\n",
    "            \n",
    "        # https://github.com/cleverhans-lab/unrolling-sgd/blob/main/Unlearning/Correlation_unlearning.py#L270\n",
    "        \n",
    "        M_unlearned = copy.deepcopy(M)\n",
    "        \n",
    "        old_params = {}\n",
    "        for i, (name, params) in enumerate(M.named_parameters()):\n",
    "            old_params[name] = params.clone()\n",
    "            for grads in grad_list:\n",
    "                old_params[name] += LR * grads[i].cuda()\n",
    "        for name, params in M_unlearned.named_parameters():\n",
    "            params.data.copy_(old_params[name])\n",
    "        \n",
    "            \n",
    "        train_time += time.time() - start_time\n",
    "\n",
    "        start_time = time.time()\n",
    "            \n",
    "        train_times.append(train_time)\n",
    "        \n",
    "        # test\n",
    "            \n",
    "        M_unlearned.eval()\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            #\n",
    "            \n",
    "            x, y = train_set.tensors\n",
    "            \n",
    "            accs = list()\n",
    "            \n",
    "            for i in range(0, x.shape[0], BATCH_SIZE):\n",
    "            \n",
    "                output = M_unlearned(x[i:i+BATCH_SIZE].cuda())\n",
    "\n",
    "                predicted = torch.argmax(output.data, dim=-1)\n",
    "                accs.append((predicted == y[i:i+BATCH_SIZE].cuda()).float().mean().detach().cpu().numpy())\n",
    "            \n",
    "            train_accs.append(np.mean(accs))\n",
    "            \n",
    "            #\n",
    "            \n",
    "            x, y = test_set.tensors\n",
    "            \n",
    "            accs = list()\n",
    "            \n",
    "            for i in range(0, x.shape[0], BATCH_SIZE):\n",
    "            \n",
    "                output = M_unlearned(x[i:i+BATCH_SIZE].cuda())\n",
    "\n",
    "                predicted = torch.argmax(output.data, dim=-1)\n",
    "                accs.append((predicted == y[i:i+BATCH_SIZE].cuda()).float().mean().detach().cpu().numpy())\n",
    "            \n",
    "            test_accs.append(np.mean(accs))\n",
    "            \n",
    "            #\n",
    "\n",
    "            x, y = forget_set.tensors\n",
    "            \n",
    "            accs = list()\n",
    "\n",
    "            for i in range(0, x.shape[0], BATCH_SIZE):\n",
    "\n",
    "                output = M_unlearned(x[i:i+BATCH_SIZE].cuda())\n",
    "\n",
    "                predicted = torch.argmax(output.data, dim=-1)\n",
    "                accs.append((predicted == y[i:i+BATCH_SIZE].cuda()).float().mean().detach().cpu().numpy())\n",
    "\n",
    "            forget_accs.append(np.mean(accs))\n",
    "        \n",
    "        # save\n",
    "        torch.save(M_unlearned.state_dict(), os.path.join(save_dir, f'{(epoch+1):03d}.pt'))\n",
    "\n",
    "    return train_times, train_accs, test_accs, forget_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a42f9c3-e935-41dd-bf63-d572fb99d593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/cleverhans-lab/unrolling-sgd/blob/main/Unlearning/Correlation_unlearning.py#L167\n",
    "LR = 0.01\n",
    "# https://github.com/cleverhans-lab/unrolling-sgd/blob/main/Unlearning/Correlation_unlearning.py#L72\n",
    "L2 = 0.\n",
    "# https://github.com/cleverhans-lab/unrolling-sgd/blob/main/Unlearning/Correlation_unlearning.py#L71\n",
    "STD_REG = 0.\n",
    "\n",
    "LOSS_FUNC = 'hess'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36189566-41a1-4eb2-b3f4-0a7ebf231ee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ce0762eb0564145acefc323ae132944",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = list()\n",
    "\n",
    "for percentage in tqdm(PERCENTAGES):\n",
    "    \n",
    "    model = CNN().cuda()\n",
    "    \n",
    "    model.load_state_dict(torch.load('./weights/original/005.pt'))\n",
    "    \n",
    "    train_set, test_set, forget_set = load_deleted_dataset(DATA_DIR, percentage)\n",
    "    \n",
    "    train_times, train_accs, test_accs, forget_accs = fit(model, f'weights/unrolling-sgd/correlation/{percentage}', train_set, test_set, forget_set)\n",
    "    \n",
    "    df = pd.DataFrame(zip(train_times, train_accs, test_accs, forget_accs), columns=['train_time', 'train_acc', 'test_acc', 'forget_acc'])\n",
    "    df['epoch'] = range(1, EPOCHS+1)\n",
    "    df['percentage'] = percentage\n",
    "    \n",
    "    results.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "692050cf-af0b-40f9-b6fb-428f4fa77855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>train_time</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>test_acc</th>\n",
       "      <th>forget_acc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>percentage</th>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th>1</th>\n",
       "      <td>25.764190</td>\n",
       "      <td>0.104385</td>\n",
       "      <td>0.102736</td>\n",
       "      <td>0.103070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23.685421</td>\n",
       "      <td>0.112177</td>\n",
       "      <td>0.113518</td>\n",
       "      <td>0.131031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21.660932</td>\n",
       "      <td>0.108879</td>\n",
       "      <td>0.115615</td>\n",
       "      <td>0.113487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22.418080</td>\n",
       "      <td>0.097721</td>\n",
       "      <td>0.099541</td>\n",
       "      <td>0.107456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>22.731835</td>\n",
       "      <td>0.090468</td>\n",
       "      <td>0.089257</td>\n",
       "      <td>0.084430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">10</th>\n",
       "      <th>1</th>\n",
       "      <td>21.352592</td>\n",
       "      <td>0.097267</td>\n",
       "      <td>0.098942</td>\n",
       "      <td>0.104721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21.849211</td>\n",
       "      <td>0.098341</td>\n",
       "      <td>0.095747</td>\n",
       "      <td>0.101729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21.849150</td>\n",
       "      <td>0.111708</td>\n",
       "      <td>0.112919</td>\n",
       "      <td>0.111868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21.583047</td>\n",
       "      <td>0.102247</td>\n",
       "      <td>0.101038</td>\n",
       "      <td>0.101230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>24.558183</td>\n",
       "      <td>0.103951</td>\n",
       "      <td>0.102736</td>\n",
       "      <td>0.108544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">20</th>\n",
       "      <th>1</th>\n",
       "      <td>24.163665</td>\n",
       "      <td>0.103937</td>\n",
       "      <td>0.101138</td>\n",
       "      <td>0.098250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22.754565</td>\n",
       "      <td>0.102146</td>\n",
       "      <td>0.101038</td>\n",
       "      <td>0.102333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22.885594</td>\n",
       "      <td>0.073292</td>\n",
       "      <td>0.074281</td>\n",
       "      <td>0.065917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23.689296</td>\n",
       "      <td>0.099583</td>\n",
       "      <td>0.100739</td>\n",
       "      <td>0.099667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>22.732578</td>\n",
       "      <td>0.229479</td>\n",
       "      <td>0.235823</td>\n",
       "      <td>0.231083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">30</th>\n",
       "      <th>1</th>\n",
       "      <td>22.394233</td>\n",
       "      <td>0.091299</td>\n",
       "      <td>0.089257</td>\n",
       "      <td>0.088088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22.529638</td>\n",
       "      <td>0.134592</td>\n",
       "      <td>0.129892</td>\n",
       "      <td>0.132549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22.501310</td>\n",
       "      <td>0.188619</td>\n",
       "      <td>0.185703</td>\n",
       "      <td>0.185058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22.656119</td>\n",
       "      <td>0.064666</td>\n",
       "      <td>0.062899</td>\n",
       "      <td>0.067607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>23.018073</td>\n",
       "      <td>0.144112</td>\n",
       "      <td>0.146066</td>\n",
       "      <td>0.142651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">40</th>\n",
       "      <th>1</th>\n",
       "      <td>24.329224</td>\n",
       "      <td>0.104889</td>\n",
       "      <td>0.102736</td>\n",
       "      <td>0.103708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23.939088</td>\n",
       "      <td>0.098611</td>\n",
       "      <td>0.101737</td>\n",
       "      <td>0.100708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23.193644</td>\n",
       "      <td>0.098278</td>\n",
       "      <td>0.100839</td>\n",
       "      <td>0.100458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23.647509</td>\n",
       "      <td>0.111556</td>\n",
       "      <td>0.113518</td>\n",
       "      <td>0.114042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>23.828699</td>\n",
       "      <td>0.116167</td>\n",
       "      <td>0.109625</td>\n",
       "      <td>0.115667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">50</th>\n",
       "      <th>1</th>\n",
       "      <td>25.336387</td>\n",
       "      <td>0.122668</td>\n",
       "      <td>0.129293</td>\n",
       "      <td>0.127265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23.750583</td>\n",
       "      <td>0.109841</td>\n",
       "      <td>0.115016</td>\n",
       "      <td>0.109375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23.896306</td>\n",
       "      <td>0.153851</td>\n",
       "      <td>0.154852</td>\n",
       "      <td>0.152452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24.098047</td>\n",
       "      <td>0.105244</td>\n",
       "      <td>0.102736</td>\n",
       "      <td>0.103578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>23.947850</td>\n",
       "      <td>0.116671</td>\n",
       "      <td>0.120208</td>\n",
       "      <td>0.120502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">60</th>\n",
       "      <th>1</th>\n",
       "      <td>23.459184</td>\n",
       "      <td>0.096875</td>\n",
       "      <td>0.100839</td>\n",
       "      <td>0.100667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23.363450</td>\n",
       "      <td>0.108042</td>\n",
       "      <td>0.115216</td>\n",
       "      <td>0.112222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23.397421</td>\n",
       "      <td>0.123167</td>\n",
       "      <td>0.121506</td>\n",
       "      <td>0.118639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23.583306</td>\n",
       "      <td>0.256125</td>\n",
       "      <td>0.258786</td>\n",
       "      <td>0.259000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>24.104743</td>\n",
       "      <td>0.146750</td>\n",
       "      <td>0.149361</td>\n",
       "      <td>0.144306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">70</th>\n",
       "      <th>1</th>\n",
       "      <td>24.747838</td>\n",
       "      <td>0.096470</td>\n",
       "      <td>0.099740</td>\n",
       "      <td>0.100819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23.775182</td>\n",
       "      <td>0.103464</td>\n",
       "      <td>0.101038</td>\n",
       "      <td>0.101604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23.917100</td>\n",
       "      <td>0.107960</td>\n",
       "      <td>0.113918</td>\n",
       "      <td>0.114314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23.781101</td>\n",
       "      <td>0.133659</td>\n",
       "      <td>0.125300</td>\n",
       "      <td>0.128332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>23.847407</td>\n",
       "      <td>0.189443</td>\n",
       "      <td>0.191693</td>\n",
       "      <td>0.194664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">80</th>\n",
       "      <th>1</th>\n",
       "      <td>24.722005</td>\n",
       "      <td>0.095083</td>\n",
       "      <td>0.091254</td>\n",
       "      <td>0.093396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24.991220</td>\n",
       "      <td>0.097500</td>\n",
       "      <td>0.095847</td>\n",
       "      <td>0.099521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24.967969</td>\n",
       "      <td>0.251583</td>\n",
       "      <td>0.246206</td>\n",
       "      <td>0.242125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24.173449</td>\n",
       "      <td>0.556833</td>\n",
       "      <td>0.566094</td>\n",
       "      <td>0.561292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>24.717310</td>\n",
       "      <td>0.472333</td>\n",
       "      <td>0.482129</td>\n",
       "      <td>0.477396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">90</th>\n",
       "      <th>1</th>\n",
       "      <td>25.163023</td>\n",
       "      <td>0.644614</td>\n",
       "      <td>0.640875</td>\n",
       "      <td>0.639977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26.689583</td>\n",
       "      <td>0.428856</td>\n",
       "      <td>0.431310</td>\n",
       "      <td>0.422375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25.583513</td>\n",
       "      <td>0.390459</td>\n",
       "      <td>0.386981</td>\n",
       "      <td>0.386423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24.881740</td>\n",
       "      <td>0.870346</td>\n",
       "      <td>0.870108</td>\n",
       "      <td>0.870761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>25.125331</td>\n",
       "      <td>0.768949</td>\n",
       "      <td>0.772364</td>\n",
       "      <td>0.771438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">99</th>\n",
       "      <th>1</th>\n",
       "      <td>25.574194</td>\n",
       "      <td>0.970395</td>\n",
       "      <td>0.968650</td>\n",
       "      <td>0.968127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25.713386</td>\n",
       "      <td>0.965461</td>\n",
       "      <td>0.969848</td>\n",
       "      <td>0.969608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25.140595</td>\n",
       "      <td>0.981908</td>\n",
       "      <td>0.987520</td>\n",
       "      <td>0.987850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25.816537</td>\n",
       "      <td>0.983553</td>\n",
       "      <td>0.985823</td>\n",
       "      <td>0.987396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>25.232232</td>\n",
       "      <td>0.988487</td>\n",
       "      <td>0.988319</td>\n",
       "      <td>0.987934</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  train_time  train_acc  test_acc  forget_acc\n",
       "percentage epoch                                             \n",
       "1          1       25.764190   0.104385  0.102736    0.103070\n",
       "           2       23.685421   0.112177  0.113518    0.131031\n",
       "           3       21.660932   0.108879  0.115615    0.113487\n",
       "           4       22.418080   0.097721  0.099541    0.107456\n",
       "           5       22.731835   0.090468  0.089257    0.084430\n",
       "10         1       21.352592   0.097267  0.098942    0.104721\n",
       "           2       21.849211   0.098341  0.095747    0.101729\n",
       "           3       21.849150   0.111708  0.112919    0.111868\n",
       "           4       21.583047   0.102247  0.101038    0.101230\n",
       "           5       24.558183   0.103951  0.102736    0.108544\n",
       "20         1       24.163665   0.103937  0.101138    0.098250\n",
       "           2       22.754565   0.102146  0.101038    0.102333\n",
       "           3       22.885594   0.073292  0.074281    0.065917\n",
       "           4       23.689296   0.099583  0.100739    0.099667\n",
       "           5       22.732578   0.229479  0.235823    0.231083\n",
       "30         1       22.394233   0.091299  0.089257    0.088088\n",
       "           2       22.529638   0.134592  0.129892    0.132549\n",
       "           3       22.501310   0.188619  0.185703    0.185058\n",
       "           4       22.656119   0.064666  0.062899    0.067607\n",
       "           5       23.018073   0.144112  0.146066    0.142651\n",
       "40         1       24.329224   0.104889  0.102736    0.103708\n",
       "           2       23.939088   0.098611  0.101737    0.100708\n",
       "           3       23.193644   0.098278  0.100839    0.100458\n",
       "           4       23.647509   0.111556  0.113518    0.114042\n",
       "           5       23.828699   0.116167  0.109625    0.115667\n",
       "50         1       25.336387   0.122668  0.129293    0.127265\n",
       "           2       23.750583   0.109841  0.115016    0.109375\n",
       "           3       23.896306   0.153851  0.154852    0.152452\n",
       "           4       24.098047   0.105244  0.102736    0.103578\n",
       "           5       23.947850   0.116671  0.120208    0.120502\n",
       "60         1       23.459184   0.096875  0.100839    0.100667\n",
       "           2       23.363450   0.108042  0.115216    0.112222\n",
       "           3       23.397421   0.123167  0.121506    0.118639\n",
       "           4       23.583306   0.256125  0.258786    0.259000\n",
       "           5       24.104743   0.146750  0.149361    0.144306\n",
       "70         1       24.747838   0.096470  0.099740    0.100819\n",
       "           2       23.775182   0.103464  0.101038    0.101604\n",
       "           3       23.917100   0.107960  0.113918    0.114314\n",
       "           4       23.781101   0.133659  0.125300    0.128332\n",
       "           5       23.847407   0.189443  0.191693    0.194664\n",
       "80         1       24.722005   0.095083  0.091254    0.093396\n",
       "           2       24.991220   0.097500  0.095847    0.099521\n",
       "           3       24.967969   0.251583  0.246206    0.242125\n",
       "           4       24.173449   0.556833  0.566094    0.561292\n",
       "           5       24.717310   0.472333  0.482129    0.477396\n",
       "90         1       25.163023   0.644614  0.640875    0.639977\n",
       "           2       26.689583   0.428856  0.431310    0.422375\n",
       "           3       25.583513   0.390459  0.386981    0.386423\n",
       "           4       24.881740   0.870346  0.870108    0.870761\n",
       "           5       25.125331   0.768949  0.772364    0.771438\n",
       "99         1       25.574194   0.970395  0.968650    0.968127\n",
       "           2       25.713386   0.965461  0.969848    0.969608\n",
       "           3       25.140595   0.981908  0.987520    0.987850\n",
       "           4       25.816537   0.983553  0.985823    0.987396\n",
       "           5       25.232232   0.988487  0.988319    0.987934"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.concat(results).set_index(['percentage', 'epoch'])\n",
    "\n",
    "results.to_csv('results/unrolling-sgd/correlation.csv')\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be47c336-f4e4-47b6-b813-bdea8d18d1e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
