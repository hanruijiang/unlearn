{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c5517c9-eab1-483c-80a2-e332fc57d686",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "import copy\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13c94faf-8d6c-44aa-9618-7192902b1933",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_original_dataset, load_deleted_dataset\n",
    "from models import CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19d524cc-f9e7-4806-af84-a4429d0e47e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'Datasets/Features/'\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5\n",
    "PERCENTAGES = [1, 10, 20, 30, 40, 50, 60, 70, 80, 90, 99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48a614e7-46f8-451b-aa48-9624b48efe77",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('results/unrolling-sgd', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabe569d-1a5f-4a7f-b90c-5144bafa7a29",
   "metadata": {},
   "source": [
    "# finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24e1d64a-eb5d-4980-ac01-73000360b291",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, save_dir, train_set, test_set, forget_set):\n",
    "    \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # prepare model\n",
    "    \n",
    "    # https://github.com/cleverhans-lab/unrolling-sgd/blob/main/Unlearning/finetune.py#L94\n",
    "    M = copy.deepcopy(model)\n",
    "    \n",
    "    # https://github.com/cleverhans-lab/unrolling-sgd/blob/main/Unlearning/finetune.py#L150\n",
    "    M_unlearned = copy.deepcopy(model)\n",
    "        \n",
    "    M_unlearned.train()\n",
    "    \n",
    "    # https://github.com/cleverhans-lab/unrolling-sgd/blob/main/Unlearning/finetune.py#L131\n",
    "    error = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    forget_loader = torch.utils.data.DataLoader(forget_set, batch_size = BATCH_SIZE, shuffle = True, drop_last=True)\n",
    "    \n",
    "    train_times = list()\n",
    "    train_accs, test_accs, forget_accs = list(), list(), list()\n",
    "    \n",
    "    for epoch in range(EPOCHS):    \n",
    "        \n",
    "        # train\n",
    "        \n",
    "        train_time = 0\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for x, y in forget_loader:\n",
    "\n",
    "            output = M_unlearned(x.cuda())\n",
    "            y = y.cuda()\n",
    "            loss = error(output, y)\n",
    "            \n",
    "            # https://github.com/cleverhans-lab/unrolling-sgd/blob/main/Unlearning/finetune.py#L163\n",
    "            loss.backward(retain_graph=True)\n",
    "            grads = torch.autograd.grad(loss, [param for param in M_unlearned.parameters()], create_graph = True)\n",
    "            \n",
    "            old_params = {}\n",
    "            for i, (name, params) in enumerate(M.named_parameters()):\n",
    "                    old_params[name] = params.clone()\n",
    "                    old_params[name] += LR * grads[i]\n",
    "            for name, params in M_unlearned.named_parameters():\n",
    "                    params.data.copy_(old_params[name])\n",
    "            \n",
    "            train_time += time.time() - start_time\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "        train_times.append(train_time)\n",
    "        \n",
    "        # test\n",
    "            \n",
    "        M_unlearned.eval()\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            #\n",
    "            \n",
    "            x, y = train_set.tensors\n",
    "            \n",
    "            accs = list()\n",
    "            \n",
    "            for i in range(0, x.shape[0], BATCH_SIZE):\n",
    "            \n",
    "                output = M_unlearned(x[i:i+BATCH_SIZE].cuda())\n",
    "\n",
    "                predicted = torch.argmax(output.data, dim=-1)\n",
    "                accs.append((predicted == y[i:i+BATCH_SIZE].cuda()).float().mean().detach().cpu().numpy())\n",
    "            \n",
    "            train_accs.append(np.mean(accs))\n",
    "            \n",
    "            #\n",
    "            \n",
    "            x, y = test_set.tensors\n",
    "            \n",
    "            accs = list()\n",
    "            \n",
    "            for i in range(0, x.shape[0], BATCH_SIZE):\n",
    "            \n",
    "                output = M_unlearned(x[i:i+BATCH_SIZE].cuda())\n",
    "\n",
    "                predicted = torch.argmax(output.data, dim=-1)\n",
    "                accs.append((predicted == y[i:i+BATCH_SIZE].cuda()).float().mean().detach().cpu().numpy())\n",
    "            \n",
    "            test_accs.append(np.mean(accs))\n",
    "            \n",
    "            #\n",
    "\n",
    "            x, y = forget_set.tensors\n",
    "\n",
    "            for i in range(0, x.shape[0], BATCH_SIZE):\n",
    "\n",
    "                output = M_unlearned(x[i:i+BATCH_SIZE].cuda())\n",
    "\n",
    "                predicted = torch.argmax(output.data, dim=-1)\n",
    "                accs.append((predicted == y[i:i+BATCH_SIZE].cuda()).float().mean().detach().cpu().numpy())\n",
    "\n",
    "            forget_accs.append(np.mean(accs))\n",
    "        \n",
    "        # save\n",
    "        torch.save(M_unlearned.state_dict(), os.path.join(save_dir, f'{(epoch+1):03d}.pt'))\n",
    "\n",
    "    return train_times, train_accs, test_accs, forget_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f92ac6b-f76c-4dde-ba0a-915d5ba15b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/cleverhans-lab/unrolling-sgd/blob/main/Unlearning/finetune.py#L49\n",
    "LR = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a81ffe7-8712-4760-b4bf-072e4d26535a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e20704ad77a417e83d5baaa5e4a4a82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = list()\n",
    "\n",
    "for percentage in tqdm(PERCENTAGES):\n",
    "    \n",
    "    model = CNN().cuda()\n",
    "    \n",
    "    model.load_state_dict(torch.load('./weights/original/005.pt'))\n",
    "    \n",
    "    train_set, test_set, forget_set = load_deleted_dataset(DATA_DIR, percentage)\n",
    "    \n",
    "    train_times, train_accs, test_accs, forget_accs = fit(model, f'weights/unrolling-sgd/finetune/{percentage}', train_set, test_set, forget_set)\n",
    "    \n",
    "    df = pd.DataFrame(zip(train_times, train_accs, test_accs, forget_accs), columns=['train_time', 'train_acc', 'test_acc', 'forget_acc'])\n",
    "    df['epoch'] = range(1, EPOCHS+1)\n",
    "    df['percentage'] = percentage\n",
    "    \n",
    "    results.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f09bcabd-676b-4172-ad65-1ba2117bd35c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>train_time</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>test_acc</th>\n",
       "      <th>forget_acc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>percentage</th>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th>1</th>\n",
       "      <td>0.114819</td>\n",
       "      <td>0.098697</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.097892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.115559</td>\n",
       "      <td>0.098697</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.097892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.114927</td>\n",
       "      <td>0.098697</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.097892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.113807</td>\n",
       "      <td>0.098697</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.097892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.115245</td>\n",
       "      <td>0.098697</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.097892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">10</th>\n",
       "      <th>1</th>\n",
       "      <td>1.623883</td>\n",
       "      <td>0.098711</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.676104</td>\n",
       "      <td>0.098711</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.707427</td>\n",
       "      <td>0.098711</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.522140</td>\n",
       "      <td>0.098711</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.667175</td>\n",
       "      <td>0.098711</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">20</th>\n",
       "      <th>1</th>\n",
       "      <td>3.686478</td>\n",
       "      <td>0.098271</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.099337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.319183</td>\n",
       "      <td>0.098271</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.099337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.514916</td>\n",
       "      <td>0.098271</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.099337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.503710</td>\n",
       "      <td>0.098271</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.099337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.320124</td>\n",
       "      <td>0.098271</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.099337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">30</th>\n",
       "      <th>1</th>\n",
       "      <td>5.456588</td>\n",
       "      <td>0.098558</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.144800</td>\n",
       "      <td>0.098558</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.249419</td>\n",
       "      <td>0.098558</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.227467</td>\n",
       "      <td>0.098558</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.495168</td>\n",
       "      <td>0.098558</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">40</th>\n",
       "      <th>1</th>\n",
       "      <td>7.273762</td>\n",
       "      <td>0.098389</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.052521</td>\n",
       "      <td>0.098389</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.937870</td>\n",
       "      <td>0.098389</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.113247</td>\n",
       "      <td>0.098389</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.694447</td>\n",
       "      <td>0.098389</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">50</th>\n",
       "      <th>1</th>\n",
       "      <td>9.331224</td>\n",
       "      <td>0.098714</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.598808</td>\n",
       "      <td>0.098714</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.630263</td>\n",
       "      <td>0.098714</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.774745</td>\n",
       "      <td>0.098714</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8.850331</td>\n",
       "      <td>0.098714</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">60</th>\n",
       "      <th>1</th>\n",
       "      <td>11.369717</td>\n",
       "      <td>0.099542</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.553390</td>\n",
       "      <td>0.099542</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.570448</td>\n",
       "      <td>0.099542</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.481780</td>\n",
       "      <td>0.099542</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10.458821</td>\n",
       "      <td>0.099542</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">70</th>\n",
       "      <th>1</th>\n",
       "      <td>13.416203</td>\n",
       "      <td>0.098857</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.215645</td>\n",
       "      <td>0.098857</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12.384325</td>\n",
       "      <td>0.098857</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.393462</td>\n",
       "      <td>0.098857</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12.267377</td>\n",
       "      <td>0.098857</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">80</th>\n",
       "      <th>1</th>\n",
       "      <td>15.650421</td>\n",
       "      <td>0.099500</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14.079730</td>\n",
       "      <td>0.099500</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.104240</td>\n",
       "      <td>0.099500</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14.034564</td>\n",
       "      <td>0.099500</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14.322129</td>\n",
       "      <td>0.099500</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">90</th>\n",
       "      <th>1</th>\n",
       "      <td>17.752018</td>\n",
       "      <td>0.097739</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.812947</td>\n",
       "      <td>0.097739</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.177165</td>\n",
       "      <td>0.097739</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.289349</td>\n",
       "      <td>0.097739</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>15.677236</td>\n",
       "      <td>0.097739</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">99</th>\n",
       "      <th>1</th>\n",
       "      <td>18.545673</td>\n",
       "      <td>0.095943</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17.629697</td>\n",
       "      <td>0.095943</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17.272803</td>\n",
       "      <td>0.095943</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.738442</td>\n",
       "      <td>0.095943</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>17.719019</td>\n",
       "      <td>0.095943</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098589</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  train_time  train_acc  test_acc  forget_acc\n",
       "percentage epoch                                             \n",
       "1          1        0.114819   0.098697  0.097943    0.097892\n",
       "           2        0.115559   0.098697  0.097943    0.097892\n",
       "           3        0.114927   0.098697  0.097943    0.097892\n",
       "           4        0.113807   0.098697  0.097943    0.097892\n",
       "           5        0.115245   0.098697  0.097943    0.097892\n",
       "10         1        1.623883   0.098711  0.097943    0.098116\n",
       "           2        1.676104   0.098711  0.097943    0.098116\n",
       "           3        1.707427   0.098711  0.097943    0.098116\n",
       "           4        1.522140   0.098711  0.097943    0.098116\n",
       "           5        1.667175   0.098711  0.097943    0.098116\n",
       "20         1        3.686478   0.098271  0.097943    0.099337\n",
       "           2        3.319183   0.098271  0.097943    0.099337\n",
       "           3        3.514916   0.098271  0.097943    0.099337\n",
       "           4        3.503710   0.098271  0.097943    0.099337\n",
       "           5        3.320124   0.098271  0.097943    0.099337\n",
       "30         1        5.456588   0.098558  0.097943    0.098637\n",
       "           2        5.144800   0.098558  0.097943    0.098637\n",
       "           3        5.249419   0.098558  0.097943    0.098637\n",
       "           4        5.227467   0.098558  0.097943    0.098637\n",
       "           5        5.495168   0.098558  0.097943    0.098637\n",
       "40         1        7.273762   0.098389  0.097943    0.098836\n",
       "           2        7.052521   0.098389  0.097943    0.098836\n",
       "           3        6.937870   0.098389  0.097943    0.098836\n",
       "           4        7.113247   0.098389  0.097943    0.098836\n",
       "           5        6.694447   0.098389  0.097943    0.098836\n",
       "50         1        9.331224   0.098714  0.097943    0.098521\n",
       "           2        8.598808   0.098714  0.097943    0.098521\n",
       "           3        8.630263   0.098714  0.097943    0.098521\n",
       "           4        8.774745   0.098714  0.097943    0.098521\n",
       "           5        8.850331   0.098714  0.097943    0.098521\n",
       "60         1       11.369717   0.099542  0.097943    0.098118\n",
       "           2       10.553390   0.099542  0.097943    0.098118\n",
       "           3       10.570448   0.099542  0.097943    0.098118\n",
       "           4       10.481780   0.099542  0.097943    0.098118\n",
       "           5       10.458821   0.099542  0.097943    0.098118\n",
       "70         1       13.416203   0.098857  0.097943    0.098536\n",
       "           2       12.215645   0.098857  0.097943    0.098536\n",
       "           3       12.384325   0.098857  0.097943    0.098536\n",
       "           4       12.393462   0.098857  0.097943    0.098536\n",
       "           5       12.267377   0.098857  0.097943    0.098536\n",
       "80         1       15.650421   0.099500  0.097943    0.098421\n",
       "           2       14.079730   0.099500  0.097943    0.098421\n",
       "           3       14.104240   0.099500  0.097943    0.098421\n",
       "           4       14.034564   0.099500  0.097943    0.098421\n",
       "           5       14.322129   0.099500  0.097943    0.098421\n",
       "90         1       17.752018   0.097739  0.097943    0.098669\n",
       "           2       15.812947   0.097739  0.097943    0.098669\n",
       "           3       16.177165   0.097739  0.097943    0.098669\n",
       "           4       16.289349   0.097739  0.097943    0.098669\n",
       "           5       15.677236   0.097739  0.097943    0.098669\n",
       "99         1       18.545673   0.095943  0.097943    0.098589\n",
       "           2       17.629697   0.095943  0.097943    0.098589\n",
       "           3       17.272803   0.095943  0.097943    0.098589\n",
       "           4       17.738442   0.095943  0.097943    0.098589\n",
       "           5       17.719019   0.095943  0.097943    0.098589"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.concat(results).set_index(['percentage', 'epoch'])\n",
    "\n",
    "results.to_csv('results/unrolling-sgd/finetune.csv')\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1186c623-c225-4167-945e-9f9239d0f59a",
   "metadata": {},
   "source": [
    "# correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a47633c-9ee2-4e99-90f1-0c52b74f0016",
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_loss(x, y):\n",
    "    log_prob = -1.0 * F.log_softmax(x, 1)\n",
    "    loss = log_prob.gather(1, y.unsqueeze(1))\n",
    "    loss = loss.mean()\n",
    "    avg_std = torch.sum(torch.std(x, dim=1))/(len(x.view(-1)))\n",
    "    loss = loss + STD_REG*avg_std\n",
    "    return loss\n",
    "\n",
    "def my_cross_entropy(x, y):\n",
    "    log_prob = -1.0 * F.log_softmax(x, 1)\n",
    "    loss = log_prob.gather(1, y.unsqueeze(1))\n",
    "    loss = loss.mean()\n",
    "\n",
    "    #x is (N,C)\n",
    "    N,C = x.shape\n",
    "    p = F.softmax(x,1)\n",
    "    hessian_loss = torch.sum(p *(1-p),dim =1)\n",
    "    hessian_loss = hessian_loss.mean()\n",
    "\n",
    "    loss = loss + STD_REG * hessian_loss\n",
    "    return loss\n",
    "\n",
    "# https://github.com/cleverhans-lab/unrolling-sgd/blob/main/Unlearning/Correlation_unlearning.py#L130\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9368b9e-4fdf-47ee-bb07-dae3d7e6512f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, save_dir, train_set, test_set, forget_set):\n",
    "    \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # https://github.com/cleverhans-lab/unrolling-sgd/blob/main/Unlearning/Correlation_unlearning.py#L161\n",
    "\n",
    "    M = copy.deepcopy(model)\n",
    "\n",
    "    M.train()\n",
    "    \n",
    "    # https://github.com/cleverhans-lab/unrolling-sgd/blob/main/Unlearning/Correlation_unlearning.py#L172\n",
    "    optimizer = torch.optim.SGD(M.parameters(), lr=LR, weight_decay = L2)\n",
    "    \n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size = BATCH_SIZE, shuffle = True, drop_last=True)\n",
    "    \n",
    "    forget_loader = torch.utils.data.DataLoader(forget_set, batch_size = BATCH_SIZE, shuffle = True, drop_last=True)\n",
    "    \n",
    "    train_times = list()\n",
    "    train_accs, test_accs, forget_accs = list(), list(), list()\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        \n",
    "        # train\n",
    "        \n",
    "        train_time = 0\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # https://github.com/cleverhans-lab/unrolling-sgd/blob/main/Unlearning/Correlation_unlearning.py#L200\n",
    "        \n",
    "        # only update M:\n",
    "        \n",
    "        M.train()\n",
    "        \n",
    "        for x, y in forget_loader:\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs_M = M(x.cuda())\n",
    "            \n",
    "            targets = y.cuda()\n",
    "        \n",
    "            if LOSS_FUNC == 'hess':\n",
    "                loss_M = my_cross_entropy(outputs_M, targets)\n",
    "            if LOSS_FUNC =='std':\n",
    "                loss_M = std_loss(outputs_M, targets)\n",
    "            if LOSS_FUNC =='regular':\n",
    "                loss_M = loss_fn(outputs_M, targets)\n",
    "                \n",
    "            loss_M.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # now we also want to compute the gradient:\n",
    "        \n",
    "        grad_list = []\n",
    "        \n",
    "        for x, y in train_loader:\n",
    "\n",
    "            output_grad = M(x.cuda())\n",
    "            \n",
    "            label = y.cuda()\n",
    "            \n",
    "            if LOSS_FUNC == 'hess':\n",
    "                loss_grad = my_cross_entropy(output_grad, label)\n",
    "            if LOSS_FUNC =='std':\n",
    "                loss_grad = std_loss(output_grad, label)\n",
    "            if LOSS_FUNC =='regular':\n",
    "                loss_grad = loss_fn(output_grad, label)\n",
    "            \n",
    "            loss_grad.backward(retain_graph=True)\n",
    "            grads = torch.autograd.grad(loss_grad, [param for param in M.parameters()], create_graph = True)\n",
    "            # !!! modified !!! original code cause OOM\n",
    "            # grad_list.append(grads)\n",
    "            grad_list.append([i.detach().cpu() for i in grads])\n",
    "        \n",
    "        # https://github.com/cleverhans-lab/unrolling-sgd/blob/main/Unlearning/Correlation_unlearning.py#L227\n",
    "            \n",
    "        # update both M and M'\n",
    "        \n",
    "        for x, y in forget_loader:\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs_M = M(x.cuda())\n",
    "            \n",
    "            targets = y.cuda()\n",
    "        \n",
    "            if LOSS_FUNC == 'hess':\n",
    "                loss_M = my_cross_entropy(outputs_M, targets)\n",
    "            if LOSS_FUNC =='std':\n",
    "                loss_M = std_loss(outputs_M, targets)\n",
    "            if LOSS_FUNC =='regular':\n",
    "                loss_M = loss_fn(outputs_M, targets)\n",
    "                \n",
    "            loss_M.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        # Now, get M''_(N+t)\n",
    "            \n",
    "        # https://github.com/cleverhans-lab/unrolling-sgd/blob/main/Unlearning/Correlation_unlearning.py#L270\n",
    "        \n",
    "        M_unlearned = copy.deepcopy(M)\n",
    "        \n",
    "        old_params = {}\n",
    "        for i, (name, params) in enumerate(M.named_parameters()):\n",
    "            old_params[name] = params.clone()\n",
    "            for grads in grad_list:\n",
    "                old_params[name] += LR * grads[i].cuda()\n",
    "        for name, params in M_unlearned.named_parameters():\n",
    "            params.data.copy_(old_params[name])\n",
    "        \n",
    "            \n",
    "        train_time += time.time() - start_time\n",
    "\n",
    "        start_time = time.time()\n",
    "            \n",
    "        train_times.append(train_time)\n",
    "        \n",
    "        # test\n",
    "            \n",
    "        M_unlearned.eval()\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            #\n",
    "            \n",
    "            x, y = train_set.tensors\n",
    "            \n",
    "            accs = list()\n",
    "            \n",
    "            for i in range(0, x.shape[0], BATCH_SIZE):\n",
    "            \n",
    "                output = M_unlearned(x[i:i+BATCH_SIZE].cuda())\n",
    "\n",
    "                predicted = torch.argmax(output.data, dim=-1)\n",
    "                accs.append((predicted == y[i:i+BATCH_SIZE].cuda()).float().mean().detach().cpu().numpy())\n",
    "            \n",
    "            train_accs.append(np.mean(accs))\n",
    "            \n",
    "            #\n",
    "            \n",
    "            x, y = test_set.tensors\n",
    "            \n",
    "            accs = list()\n",
    "            \n",
    "            for i in range(0, x.shape[0], BATCH_SIZE):\n",
    "            \n",
    "                output = M_unlearned(x[i:i+BATCH_SIZE].cuda())\n",
    "\n",
    "                predicted = torch.argmax(output.data, dim=-1)\n",
    "                accs.append((predicted == y[i:i+BATCH_SIZE].cuda()).float().mean().detach().cpu().numpy())\n",
    "            \n",
    "            test_accs.append(np.mean(accs))\n",
    "            \n",
    "            #\n",
    "\n",
    "            x, y = forget_set.tensors\n",
    "\n",
    "            for i in range(0, x.shape[0], BATCH_SIZE):\n",
    "\n",
    "                output = M_unlearned(x[i:i+BATCH_SIZE].cuda())\n",
    "\n",
    "                predicted = torch.argmax(output.data, dim=-1)\n",
    "                accs.append((predicted == y[i:i+BATCH_SIZE].cuda()).float().mean().detach().cpu().numpy())\n",
    "\n",
    "            forget_accs.append(np.mean(accs))\n",
    "        \n",
    "        # save\n",
    "        torch.save(M_unlearned.state_dict(), os.path.join(save_dir, f'{(epoch+1):03d}.pt'))\n",
    "\n",
    "    return train_times, train_accs, test_accs, forget_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a42f9c3-e935-41dd-bf63-d572fb99d593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/cleverhans-lab/unrolling-sgd/blob/main/Unlearning/Correlation_unlearning.py#L167\n",
    "LR = 0.01\n",
    "# https://github.com/cleverhans-lab/unrolling-sgd/blob/main/Unlearning/Correlation_unlearning.py#L72\n",
    "L2 = 0.\n",
    "# https://github.com/cleverhans-lab/unrolling-sgd/blob/main/Unlearning/Correlation_unlearning.py#L71\n",
    "STD_REG = 0.\n",
    "\n",
    "LOSS_FUNC = 'hess'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36189566-41a1-4eb2-b3f4-0a7ebf231ee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5013c9fb0e6649acb67b1328c14dd363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = list()\n",
    "\n",
    "for percentage in tqdm(PERCENTAGES):\n",
    "    \n",
    "    model = CNN().cuda()\n",
    "    \n",
    "    model.load_state_dict(torch.load('./weights/original/005.pt'))\n",
    "    \n",
    "    train_set, test_set, forget_set = load_deleted_dataset(DATA_DIR, percentage)\n",
    "    \n",
    "    train_times, train_accs, test_accs, forget_accs = fit(model, f'weights/unrolling-sgd/correlation/{percentage}', train_set, test_set, forget_set)\n",
    "    \n",
    "    df = pd.DataFrame(zip(train_times, train_accs, test_accs, forget_accs), columns=['train_time', 'train_acc', 'test_acc', 'forget_acc'])\n",
    "    df['epoch'] = range(1, EPOCHS+1)\n",
    "    df['percentage'] = percentage\n",
    "    \n",
    "    results.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "692050cf-af0b-40f9-b6fb-428f4fa77855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>train_time</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>test_acc</th>\n",
       "      <th>forget_acc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>percentage</th>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th>1</th>\n",
       "      <td>24.607036</td>\n",
       "      <td>0.112177</td>\n",
       "      <td>0.113518</td>\n",
       "      <td>0.114521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22.409146</td>\n",
       "      <td>0.104385</td>\n",
       "      <td>0.102736</td>\n",
       "      <td>0.102755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21.495975</td>\n",
       "      <td>0.090805</td>\n",
       "      <td>0.089357</td>\n",
       "      <td>0.089075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21.650668</td>\n",
       "      <td>0.102231</td>\n",
       "      <td>0.101038</td>\n",
       "      <td>0.100841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>21.699512</td>\n",
       "      <td>0.098176</td>\n",
       "      <td>0.094549</td>\n",
       "      <td>0.094346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">10</th>\n",
       "      <th>1</th>\n",
       "      <td>22.453834</td>\n",
       "      <td>0.112430</td>\n",
       "      <td>0.113518</td>\n",
       "      <td>0.112837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21.562140</td>\n",
       "      <td>0.096620</td>\n",
       "      <td>0.098243</td>\n",
       "      <td>0.100299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22.439224</td>\n",
       "      <td>0.102599</td>\n",
       "      <td>0.103734</td>\n",
       "      <td>0.105352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21.875947</td>\n",
       "      <td>0.094676</td>\n",
       "      <td>0.096745</td>\n",
       "      <td>0.098802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>22.301778</td>\n",
       "      <td>0.096564</td>\n",
       "      <td>0.098243</td>\n",
       "      <td>0.100611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">20</th>\n",
       "      <th>1</th>\n",
       "      <td>22.122407</td>\n",
       "      <td>0.064917</td>\n",
       "      <td>0.068091</td>\n",
       "      <td>0.063227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22.292046</td>\n",
       "      <td>0.102146</td>\n",
       "      <td>0.101038</td>\n",
       "      <td>0.101744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22.070628</td>\n",
       "      <td>0.132979</td>\n",
       "      <td>0.135483</td>\n",
       "      <td>0.138399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22.459756</td>\n",
       "      <td>0.126500</td>\n",
       "      <td>0.121106</td>\n",
       "      <td>0.125454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>22.792111</td>\n",
       "      <td>0.142312</td>\n",
       "      <td>0.140176</td>\n",
       "      <td>0.136537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">30</th>\n",
       "      <th>1</th>\n",
       "      <td>22.798678</td>\n",
       "      <td>0.101128</td>\n",
       "      <td>0.101038</td>\n",
       "      <td>0.103382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22.763214</td>\n",
       "      <td>0.103913</td>\n",
       "      <td>0.102736</td>\n",
       "      <td>0.104488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22.449966</td>\n",
       "      <td>0.113600</td>\n",
       "      <td>0.116713</td>\n",
       "      <td>0.117330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22.867168</td>\n",
       "      <td>0.106245</td>\n",
       "      <td>0.105431</td>\n",
       "      <td>0.104274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>23.421859</td>\n",
       "      <td>0.099058</td>\n",
       "      <td>0.093850</td>\n",
       "      <td>0.096675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">40</th>\n",
       "      <th>1</th>\n",
       "      <td>23.278412</td>\n",
       "      <td>0.188500</td>\n",
       "      <td>0.191394</td>\n",
       "      <td>0.191792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22.885220</td>\n",
       "      <td>0.091028</td>\n",
       "      <td>0.089257</td>\n",
       "      <td>0.089311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23.720358</td>\n",
       "      <td>0.123417</td>\n",
       "      <td>0.126198</td>\n",
       "      <td>0.126176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23.848705</td>\n",
       "      <td>0.104889</td>\n",
       "      <td>0.102736</td>\n",
       "      <td>0.103393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>22.565082</td>\n",
       "      <td>0.097083</td>\n",
       "      <td>0.098243</td>\n",
       "      <td>0.097925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">50</th>\n",
       "      <th>1</th>\n",
       "      <td>23.553720</td>\n",
       "      <td>0.097148</td>\n",
       "      <td>0.098243</td>\n",
       "      <td>0.097697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24.334825</td>\n",
       "      <td>0.098647</td>\n",
       "      <td>0.101438</td>\n",
       "      <td>0.100969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23.938523</td>\n",
       "      <td>0.178738</td>\n",
       "      <td>0.187200</td>\n",
       "      <td>0.183453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24.299111</td>\n",
       "      <td>0.101912</td>\n",
       "      <td>0.101038</td>\n",
       "      <td>0.102068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>24.088158</td>\n",
       "      <td>0.098981</td>\n",
       "      <td>0.102037</td>\n",
       "      <td>0.097922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">60</th>\n",
       "      <th>1</th>\n",
       "      <td>24.448012</td>\n",
       "      <td>0.101708</td>\n",
       "      <td>0.101038</td>\n",
       "      <td>0.102182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23.686167</td>\n",
       "      <td>0.229000</td>\n",
       "      <td>0.225739</td>\n",
       "      <td>0.225726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23.894461</td>\n",
       "      <td>0.191500</td>\n",
       "      <td>0.193191</td>\n",
       "      <td>0.193976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23.358061</td>\n",
       "      <td>0.115500</td>\n",
       "      <td>0.111322</td>\n",
       "      <td>0.113547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>23.952212</td>\n",
       "      <td>0.149083</td>\n",
       "      <td>0.144069</td>\n",
       "      <td>0.151752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">70</th>\n",
       "      <th>1</th>\n",
       "      <td>24.385867</td>\n",
       "      <td>0.198657</td>\n",
       "      <td>0.202676</td>\n",
       "      <td>0.205028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24.623375</td>\n",
       "      <td>0.101965</td>\n",
       "      <td>0.098043</td>\n",
       "      <td>0.097632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23.974000</td>\n",
       "      <td>0.143928</td>\n",
       "      <td>0.144868</td>\n",
       "      <td>0.148524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24.114030</td>\n",
       "      <td>0.272647</td>\n",
       "      <td>0.264577</td>\n",
       "      <td>0.260859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>24.420247</td>\n",
       "      <td>0.551565</td>\n",
       "      <td>0.561901</td>\n",
       "      <td>0.552045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">80</th>\n",
       "      <th>1</th>\n",
       "      <td>24.597216</td>\n",
       "      <td>0.172333</td>\n",
       "      <td>0.184006</td>\n",
       "      <td>0.183053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24.288840</td>\n",
       "      <td>0.350333</td>\n",
       "      <td>0.366414</td>\n",
       "      <td>0.358867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24.280944</td>\n",
       "      <td>0.115917</td>\n",
       "      <td>0.126398</td>\n",
       "      <td>0.126534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24.933414</td>\n",
       "      <td>0.249250</td>\n",
       "      <td>0.232129</td>\n",
       "      <td>0.238641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>24.853089</td>\n",
       "      <td>0.103500</td>\n",
       "      <td>0.101038</td>\n",
       "      <td>0.101713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">90</th>\n",
       "      <th>1</th>\n",
       "      <td>25.026492</td>\n",
       "      <td>0.534242</td>\n",
       "      <td>0.517971</td>\n",
       "      <td>0.518897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25.146636</td>\n",
       "      <td>0.595911</td>\n",
       "      <td>0.604832</td>\n",
       "      <td>0.600996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25.001879</td>\n",
       "      <td>0.287899</td>\n",
       "      <td>0.269369</td>\n",
       "      <td>0.270334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24.685071</td>\n",
       "      <td>0.660406</td>\n",
       "      <td>0.674022</td>\n",
       "      <td>0.670399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>25.280702</td>\n",
       "      <td>0.511469</td>\n",
       "      <td>0.502496</td>\n",
       "      <td>0.496939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">99</th>\n",
       "      <th>1</th>\n",
       "      <td>25.164451</td>\n",
       "      <td>0.095943</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25.212517</td>\n",
       "      <td>0.095943</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25.784373</td>\n",
       "      <td>0.095943</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25.652189</td>\n",
       "      <td>0.095943</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>25.334640</td>\n",
       "      <td>0.095943</td>\n",
       "      <td>0.097943</td>\n",
       "      <td>0.098589</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  train_time  train_acc  test_acc  forget_acc\n",
       "percentage epoch                                             \n",
       "1          1       24.607036   0.112177  0.113518    0.114521\n",
       "           2       22.409146   0.104385  0.102736    0.102755\n",
       "           3       21.495975   0.090805  0.089357    0.089075\n",
       "           4       21.650668   0.102231  0.101038    0.100841\n",
       "           5       21.699512   0.098176  0.094549    0.094346\n",
       "10         1       22.453834   0.112430  0.113518    0.112837\n",
       "           2       21.562140   0.096620  0.098243    0.100299\n",
       "           3       22.439224   0.102599  0.103734    0.105352\n",
       "           4       21.875947   0.094676  0.096745    0.098802\n",
       "           5       22.301778   0.096564  0.098243    0.100611\n",
       "20         1       22.122407   0.064917  0.068091    0.063227\n",
       "           2       22.292046   0.102146  0.101038    0.101744\n",
       "           3       22.070628   0.132979  0.135483    0.138399\n",
       "           4       22.459756   0.126500  0.121106    0.125454\n",
       "           5       22.792111   0.142312  0.140176    0.136537\n",
       "30         1       22.798678   0.101128  0.101038    0.103382\n",
       "           2       22.763214   0.103913  0.102736    0.104488\n",
       "           3       22.449966   0.113600  0.116713    0.117330\n",
       "           4       22.867168   0.106245  0.105431    0.104274\n",
       "           5       23.421859   0.099058  0.093850    0.096675\n",
       "40         1       23.278412   0.188500  0.191394    0.191792\n",
       "           2       22.885220   0.091028  0.089257    0.089311\n",
       "           3       23.720358   0.123417  0.126198    0.126176\n",
       "           4       23.848705   0.104889  0.102736    0.103393\n",
       "           5       22.565082   0.097083  0.098243    0.097925\n",
       "50         1       23.553720   0.097148  0.098243    0.097697\n",
       "           2       24.334825   0.098647  0.101438    0.100969\n",
       "           3       23.938523   0.178738  0.187200    0.183453\n",
       "           4       24.299111   0.101912  0.101038    0.102068\n",
       "           5       24.088158   0.098981  0.102037    0.097922\n",
       "60         1       24.448012   0.101708  0.101038    0.102182\n",
       "           2       23.686167   0.229000  0.225739    0.225726\n",
       "           3       23.894461   0.191500  0.193191    0.193976\n",
       "           4       23.358061   0.115500  0.111322    0.113547\n",
       "           5       23.952212   0.149083  0.144069    0.151752\n",
       "70         1       24.385867   0.198657  0.202676    0.205028\n",
       "           2       24.623375   0.101965  0.098043    0.097632\n",
       "           3       23.974000   0.143928  0.144868    0.148524\n",
       "           4       24.114030   0.272647  0.264577    0.260859\n",
       "           5       24.420247   0.551565  0.561901    0.552045\n",
       "80         1       24.597216   0.172333  0.184006    0.183053\n",
       "           2       24.288840   0.350333  0.366414    0.358867\n",
       "           3       24.280944   0.115917  0.126398    0.126534\n",
       "           4       24.933414   0.249250  0.232129    0.238641\n",
       "           5       24.853089   0.103500  0.101038    0.101713\n",
       "90         1       25.026492   0.534242  0.517971    0.518897\n",
       "           2       25.146636   0.595911  0.604832    0.600996\n",
       "           3       25.001879   0.287899  0.269369    0.270334\n",
       "           4       24.685071   0.660406  0.674022    0.670399\n",
       "           5       25.280702   0.511469  0.502496    0.496939\n",
       "99         1       25.164451   0.095943  0.097943    0.098589\n",
       "           2       25.212517   0.095943  0.097943    0.098589\n",
       "           3       25.784373   0.095943  0.097943    0.098589\n",
       "           4       25.652189   0.095943  0.097943    0.098589\n",
       "           5       25.334640   0.095943  0.097943    0.098589"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.concat(results).set_index(['percentage', 'epoch'])\n",
    "\n",
    "results.to_csv('results/unrolling-sgd/correlation.csv')\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4802c4-fe60-419f-8f1d-4d0665b178e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
