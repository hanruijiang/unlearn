{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16884c31-fa5b-4eb2-8f11-ad4bc417d5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import io\n",
    "\n",
    "import torch\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c2f0c1b-4f0e-486b-aacf-2b64daf963e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_original_dataset, load_deleted_dataset\n",
    "from models import CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "168fd7f3-bfe9-47cc-926f-b6a2c9e6371d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'Datasets/Features/'\n",
    "EPOCHS = 5\n",
    "PERCENTAGES = [1, 10, 20, 30, 40, 50, 60, 70, 80, 90, 99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ff142a6-533a-41d2-a147-890751aae4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath('./libraries/SISA/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b617b387-a64d-4c4c-aa15-1f20a2c8eef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/cleverhans-lab/machine-unlearning/blob/master/distribution.py#L7\n",
    "# https://github.com/cleverhans-lab/machine-unlearning/blob/master/sisa.py#L14\n",
    "\n",
    "class args:\n",
    "    distribution = \"exponential\"\n",
    "    shards = 1\n",
    "    batch_size = 16\n",
    "    learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "424ce03e-82db-4187-a471-3f187bc1e4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/cleverhans-lab/machine-unlearning/blob/master/distribution.py#L55\n",
    "\n",
    "def mass(index):\n",
    "    if args.distribution.split(\":\")[0] == \"exponential\":\n",
    "        lbd = (\n",
    "            float(args.distribution.split(\":\")[1])\n",
    "            if len(args.distribution.split(\":\")) > 1\n",
    "            else -np.log(0.05) / index.shape[0]\n",
    "        )\n",
    "        return np.exp(-lbd * index) - np.exp(-lbd * (index + 1))\n",
    "    if args.distribution.split(\":\")[0] == \"pareto\":\n",
    "        a = (\n",
    "            float(args.distribution.split(\":\")[1])\n",
    "            if len(args.distribution.split(\":\")) > 1\n",
    "            else 1.16\n",
    "        )\n",
    "        return a / ((index + 1) ** (a + 1))\n",
    "\n",
    "# https://github.com/cleverhans-lab/machine-unlearning/blob/master/distribution.py#L74\n",
    "\n",
    "def get_partition(nb_train):\n",
    "\n",
    "    # Initialize queue and partition.\n",
    "    weights = mass(np.arange(0, nb_train))\n",
    "    indices = np.argsort(weights)\n",
    "    queue = np.array([weights[indices], np.ones(weights.shape)]).transpose()\n",
    "    partition = [np.array([index]) for index in indices]\n",
    "    \n",
    "    # Put all points in the top queue.\n",
    "    bottom_queue = queue.shape[0]  # pylint: disable=unsubscriptable-object\n",
    "    lim = (\n",
    "        int(float(args.distribution.split(\":\")[1]) * nb_train)\n",
    "        if len(args.distribution.split(\":\")) > 1\n",
    "        else int(0.01 * nb_train)\n",
    "    )\n",
    "    \n",
    "    for _ in range(nb_train - args.shards):\n",
    "        # Fetch top 2 clusters and merge them.\n",
    "        w1 = queue[0]\n",
    "        w2 = queue[1]\n",
    "    \n",
    "        l1 = partition[0]\n",
    "        l2 = partition[1]\n",
    "    \n",
    "        partition = partition[2:]\n",
    "        queue = queue[2:]\n",
    "        bottom_queue -= 2\n",
    "    \n",
    "        merged_weight = w1 + w2\n",
    "    \n",
    "        # If merged cluster is smaller in number of points than the limit, insert it in top queue.\n",
    "        if merged_weight[1] < lim:\n",
    "            # Top queue is ordered first by number of points (weight[1]) and second by cost (weight[0]).\n",
    "            offset_array = np.where(queue[:bottom_queue, 1] >= merged_weight[1])\n",
    "            limit_array = np.where(queue[:bottom_queue, 1] > merged_weight[1])\n",
    "            offset = (\n",
    "                offset_array[0][0]\n",
    "                if offset_array[0].shape[0] > 0\n",
    "                else bottom_queue\n",
    "            )\n",
    "            limit = (\n",
    "                limit_array[0][0]\n",
    "                if limit_array[0].shape[0] > 0\n",
    "                else bottom_queue\n",
    "            )\n",
    "            position_array = np.where(\n",
    "                queue[offset:limit][:, 0] >= merged_weight[0]\n",
    "            )\n",
    "            position = (\n",
    "                position_array[0][0]\n",
    "                if position_array[0].shape[0] > 0\n",
    "                else bottom_queue\n",
    "            )\n",
    "            bottom_queue += 1\n",
    "    \n",
    "        # Otherwise insert it in the bottom queue.\n",
    "        else:\n",
    "            # Bottom queue is ordered by cost only.\n",
    "            position_array = np.where(\n",
    "                queue[bottom_queue:][:, 0] >= merged_weight[0]\n",
    "            )\n",
    "            position = (\n",
    "                position_array[0][0]\n",
    "                if position_array[0].shape[0] > 0\n",
    "                else queue.shape[0]\n",
    "            )\n",
    "    \n",
    "        # Actual insertion.\n",
    "        queue = np.insert(queue, position, merged_weight, axis=0)\n",
    "        partition = (\n",
    "            partition[:position]\n",
    "            + [np.concatenate((l1, l2))]\n",
    "            + partition[position:]\n",
    "        )\n",
    "\n",
    "    return partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1886b80d-63fa-4a4a-a41e-6c3b9a5aa7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, save_dir, train_set, test_set, forget_set):\n",
    "    \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # 1- Create a container with a specified number of shards:\n",
    "    # https://github.com/cleverhans-lab/machine-unlearning/blob/master/example-scripts/purchase-sharding/README.txt#L5\n",
    "    # https://github.com/cleverhans-lab/machine-unlearning/blob/master/example-scripts/purchase-sharding/init.sh#L16\n",
    "\n",
    "    # run PLS-GAP algorithm to find a low cost split.\n",
    "    nb_train = len(train_set)\n",
    "    partition = get_partition(nb_train)\n",
    "    \n",
    "    # https://github.com/cleverhans-lab/machine-unlearning/blob/master/sharded.py#L37\n",
    "    train_loader = torch.utils.data.DataLoader(np.array(partition[0]), batch_size = args.batch_size, shuffle = False, drop_last=False)\n",
    "\n",
    "    # https://github.com/cleverhans-lab/machine-unlearning/blob/master/sisa.py#L98\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    # https://github.com/cleverhans-lab/machine-unlearning/blob/master/sisa.py#L100\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "    \n",
    "    train_times = list()\n",
    "    train_accs, test_accs, forget_accs = list(), list(), list()\n",
    "    # https://github.com/cleverhans-lab/machine-unlearning/blob/master/sisa.py#L183\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        \n",
    "        # train\n",
    "        \n",
    "        train_time = 0\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        for indices in train_loader:\n",
    "\n",
    "            x, y = train_set[indices]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x.cuda())\n",
    "            y = y.cuda()\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_time += time.time() - start_time\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "        train_times.append(train_time)\n",
    "        \n",
    "        # test\n",
    "            \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            x, y = train_set.tensors\n",
    "            \n",
    "            accs = list()\n",
    "            \n",
    "            for i in range(0, x.shape[0], args.batch_size):\n",
    "            \n",
    "                output = model(x[i:i+args.batch_size].cuda())\n",
    "\n",
    "                predicted = torch.argmax(output.data, dim=-1)\n",
    "                accs.append((predicted == y[i:i+args.batch_size].cuda()).float().mean().detach().cpu().numpy())\n",
    "            \n",
    "            train_accs.append(np.mean(accs))\n",
    "            \n",
    "            x, y = test_set.tensors\n",
    "            \n",
    "            accs = list()\n",
    "            \n",
    "            for i in range(0, x.shape[0], args.batch_size):\n",
    "            \n",
    "                output = model(x[i:i+args.batch_size].cuda())\n",
    "\n",
    "                predicted = torch.argmax(output.data, dim=-1)\n",
    "                accs.append((predicted == y[i:i+args.batch_size].cuda()).float().mean().detach().cpu().numpy())\n",
    "            \n",
    "            test_accs.append(np.mean(accs))\n",
    "            \n",
    "\n",
    "            x, y = forget_set.tensors\n",
    "            \n",
    "            accs = list()\n",
    "\n",
    "            for i in range(0, x.shape[0], args.batch_size):\n",
    "\n",
    "                output = model(x[i:i+args.batch_size].cuda())\n",
    "\n",
    "                predicted = torch.argmax(output.data, dim=-1)\n",
    "                accs.append((predicted == y[i:i+args.batch_size].cuda()).float().mean().detach().cpu().numpy())\n",
    "\n",
    "            forget_accs.append(np.mean(accs))\n",
    "        \n",
    "        # save\n",
    "        torch.save(model.state_dict(), os.path.join(save_dir, f'{(epoch+1):03d}.pt'))\n",
    "\n",
    "    return train_times, train_accs, test_accs, forget_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1653ceb-86fb-49aa-ba56-4e9b5bd6152b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da901f0a16c54692a17ad46ca1cb882d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = list()\n",
    "\n",
    "for percentage in tqdm(PERCENTAGES):\n",
    "    \n",
    "    model = CNN().cuda()\n",
    "\n",
    "    model.load_state_dict(torch.load('./weights/init.pt'))\n",
    "    \n",
    "    train_set, test_set, forget_set = load_deleted_dataset(DATA_DIR, percentage)\n",
    "    \n",
    "    train_times, train_accs, test_accs, forget_accs = fit(model, f'weights/SISA/{percentage}', train_set, test_set, forget_set)\n",
    "    \n",
    "    df = pd.DataFrame(zip(train_times, train_accs, test_accs, forget_accs), columns=['train_time', 'train_acc', 'test_acc', 'forget_acc'])\n",
    "    df['epoch'] = range(1, EPOCHS+1)\n",
    "    df['percentage'] = percentage\n",
    "    \n",
    "    results.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f9c872e-edac-4524-814f-8eb8d8329fd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>train_time</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>test_acc</th>\n",
       "      <th>forget_acc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>percentage</th>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th>1</th>\n",
       "      <td>12.227887</td>\n",
       "      <td>0.969230</td>\n",
       "      <td>0.9696</td>\n",
       "      <td>0.970395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11.319716</td>\n",
       "      <td>0.974835</td>\n",
       "      <td>0.9741</td>\n",
       "      <td>0.976974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.359001</td>\n",
       "      <td>0.979464</td>\n",
       "      <td>0.9788</td>\n",
       "      <td>0.980263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.324547</td>\n",
       "      <td>0.983538</td>\n",
       "      <td>0.9847</td>\n",
       "      <td>0.983553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11.355701</td>\n",
       "      <td>0.980592</td>\n",
       "      <td>0.9818</td>\n",
       "      <td>0.981908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">10</th>\n",
       "      <th>1</th>\n",
       "      <td>10.590553</td>\n",
       "      <td>0.970981</td>\n",
       "      <td>0.9733</td>\n",
       "      <td>0.970833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.380322</td>\n",
       "      <td>0.977056</td>\n",
       "      <td>0.9790</td>\n",
       "      <td>0.975167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.785652</td>\n",
       "      <td>0.979833</td>\n",
       "      <td>0.9806</td>\n",
       "      <td>0.977333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.643745</td>\n",
       "      <td>0.982222</td>\n",
       "      <td>0.9807</td>\n",
       "      <td>0.979500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10.345651</td>\n",
       "      <td>0.982241</td>\n",
       "      <td>0.9821</td>\n",
       "      <td>0.981833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">20</th>\n",
       "      <th>1</th>\n",
       "      <td>9.287708</td>\n",
       "      <td>0.970146</td>\n",
       "      <td>0.9721</td>\n",
       "      <td>0.968250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.964239</td>\n",
       "      <td>0.973000</td>\n",
       "      <td>0.9730</td>\n",
       "      <td>0.970333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.015195</td>\n",
       "      <td>0.975583</td>\n",
       "      <td>0.9748</td>\n",
       "      <td>0.969167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.998821</td>\n",
       "      <td>0.978396</td>\n",
       "      <td>0.9780</td>\n",
       "      <td>0.971833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8.972876</td>\n",
       "      <td>0.984792</td>\n",
       "      <td>0.9828</td>\n",
       "      <td>0.979917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">30</th>\n",
       "      <th>1</th>\n",
       "      <td>8.026070</td>\n",
       "      <td>0.968762</td>\n",
       "      <td>0.9694</td>\n",
       "      <td>0.965056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.835696</td>\n",
       "      <td>0.970571</td>\n",
       "      <td>0.9704</td>\n",
       "      <td>0.967056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.786371</td>\n",
       "      <td>0.979500</td>\n",
       "      <td>0.9771</td>\n",
       "      <td>0.976389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.897235</td>\n",
       "      <td>0.979190</td>\n",
       "      <td>0.9782</td>\n",
       "      <td>0.976444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7.899309</td>\n",
       "      <td>0.980048</td>\n",
       "      <td>0.9811</td>\n",
       "      <td>0.976500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">40</th>\n",
       "      <th>1</th>\n",
       "      <td>6.968718</td>\n",
       "      <td>0.959750</td>\n",
       "      <td>0.9612</td>\n",
       "      <td>0.960125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.015094</td>\n",
       "      <td>0.976389</td>\n",
       "      <td>0.9742</td>\n",
       "      <td>0.973333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.779901</td>\n",
       "      <td>0.978667</td>\n",
       "      <td>0.9765</td>\n",
       "      <td>0.975292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.845966</td>\n",
       "      <td>0.979222</td>\n",
       "      <td>0.9772</td>\n",
       "      <td>0.975667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.962294</td>\n",
       "      <td>0.981722</td>\n",
       "      <td>0.9788</td>\n",
       "      <td>0.978208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">50</th>\n",
       "      <th>1</th>\n",
       "      <td>5.672671</td>\n",
       "      <td>0.963200</td>\n",
       "      <td>0.9633</td>\n",
       "      <td>0.959700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.661997</td>\n",
       "      <td>0.976433</td>\n",
       "      <td>0.9738</td>\n",
       "      <td>0.972633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.158849</td>\n",
       "      <td>0.976867</td>\n",
       "      <td>0.9750</td>\n",
       "      <td>0.973833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.976142</td>\n",
       "      <td>0.977167</td>\n",
       "      <td>0.9749</td>\n",
       "      <td>0.973567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.810796</td>\n",
       "      <td>0.980533</td>\n",
       "      <td>0.9800</td>\n",
       "      <td>0.977167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">60</th>\n",
       "      <th>1</th>\n",
       "      <td>4.685288</td>\n",
       "      <td>0.961583</td>\n",
       "      <td>0.9603</td>\n",
       "      <td>0.958556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.430913</td>\n",
       "      <td>0.972208</td>\n",
       "      <td>0.9709</td>\n",
       "      <td>0.970472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.408038</td>\n",
       "      <td>0.975458</td>\n",
       "      <td>0.9774</td>\n",
       "      <td>0.972056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.132394</td>\n",
       "      <td>0.975625</td>\n",
       "      <td>0.9744</td>\n",
       "      <td>0.972333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.556553</td>\n",
       "      <td>0.981625</td>\n",
       "      <td>0.9819</td>\n",
       "      <td>0.977000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">70</th>\n",
       "      <th>1</th>\n",
       "      <td>3.403507</td>\n",
       "      <td>0.954833</td>\n",
       "      <td>0.9528</td>\n",
       "      <td>0.949833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.099883</td>\n",
       "      <td>0.971667</td>\n",
       "      <td>0.9708</td>\n",
       "      <td>0.966238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.997659</td>\n",
       "      <td>0.969667</td>\n",
       "      <td>0.9664</td>\n",
       "      <td>0.965929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.052327</td>\n",
       "      <td>0.976667</td>\n",
       "      <td>0.9716</td>\n",
       "      <td>0.970381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.111852</td>\n",
       "      <td>0.976222</td>\n",
       "      <td>0.9740</td>\n",
       "      <td>0.970929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">80</th>\n",
       "      <th>1</th>\n",
       "      <td>2.630246</td>\n",
       "      <td>0.945417</td>\n",
       "      <td>0.9445</td>\n",
       "      <td>0.934937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.441411</td>\n",
       "      <td>0.968417</td>\n",
       "      <td>0.9646</td>\n",
       "      <td>0.958229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.241837</td>\n",
       "      <td>0.971750</td>\n",
       "      <td>0.9648</td>\n",
       "      <td>0.961542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.417673</td>\n",
       "      <td>0.976417</td>\n",
       "      <td>0.9705</td>\n",
       "      <td>0.968333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.273694</td>\n",
       "      <td>0.981417</td>\n",
       "      <td>0.9745</td>\n",
       "      <td>0.970792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">90</th>\n",
       "      <th>1</th>\n",
       "      <td>1.100159</td>\n",
       "      <td>0.927833</td>\n",
       "      <td>0.9118</td>\n",
       "      <td>0.899648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.134328</td>\n",
       "      <td>0.945833</td>\n",
       "      <td>0.9259</td>\n",
       "      <td>0.920204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.138127</td>\n",
       "      <td>0.962667</td>\n",
       "      <td>0.9441</td>\n",
       "      <td>0.939704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.115206</td>\n",
       "      <td>0.975167</td>\n",
       "      <td>0.9613</td>\n",
       "      <td>0.957167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.122202</td>\n",
       "      <td>0.977833</td>\n",
       "      <td>0.9610</td>\n",
       "      <td>0.956981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">99</th>\n",
       "      <th>1</th>\n",
       "      <td>0.108739</td>\n",
       "      <td>0.245066</td>\n",
       "      <td>0.1919</td>\n",
       "      <td>0.187012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.105728</td>\n",
       "      <td>0.697368</td>\n",
       "      <td>0.6024</td>\n",
       "      <td>0.597074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.123367</td>\n",
       "      <td>0.858553</td>\n",
       "      <td>0.7562</td>\n",
       "      <td>0.746785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.106161</td>\n",
       "      <td>0.888158</td>\n",
       "      <td>0.7833</td>\n",
       "      <td>0.779407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.161261</td>\n",
       "      <td>0.896382</td>\n",
       "      <td>0.7764</td>\n",
       "      <td>0.765570</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  train_time  train_acc  test_acc  forget_acc\n",
       "percentage epoch                                             \n",
       "1          1       12.227887   0.969230    0.9696    0.970395\n",
       "           2       11.319716   0.974835    0.9741    0.976974\n",
       "           3       11.359001   0.979464    0.9788    0.980263\n",
       "           4       11.324547   0.983538    0.9847    0.983553\n",
       "           5       11.355701   0.980592    0.9818    0.981908\n",
       "10         1       10.590553   0.970981    0.9733    0.970833\n",
       "           2       10.380322   0.977056    0.9790    0.975167\n",
       "           3       10.785652   0.979833    0.9806    0.977333\n",
       "           4       10.643745   0.982222    0.9807    0.979500\n",
       "           5       10.345651   0.982241    0.9821    0.981833\n",
       "20         1        9.287708   0.970146    0.9721    0.968250\n",
       "           2        8.964239   0.973000    0.9730    0.970333\n",
       "           3        9.015195   0.975583    0.9748    0.969167\n",
       "           4        8.998821   0.978396    0.9780    0.971833\n",
       "           5        8.972876   0.984792    0.9828    0.979917\n",
       "30         1        8.026070   0.968762    0.9694    0.965056\n",
       "           2        7.835696   0.970571    0.9704    0.967056\n",
       "           3        7.786371   0.979500    0.9771    0.976389\n",
       "           4        7.897235   0.979190    0.9782    0.976444\n",
       "           5        7.899309   0.980048    0.9811    0.976500\n",
       "40         1        6.968718   0.959750    0.9612    0.960125\n",
       "           2        7.015094   0.976389    0.9742    0.973333\n",
       "           3        6.779901   0.978667    0.9765    0.975292\n",
       "           4        6.845966   0.979222    0.9772    0.975667\n",
       "           5        6.962294   0.981722    0.9788    0.978208\n",
       "50         1        5.672671   0.963200    0.9633    0.959700\n",
       "           2        5.661997   0.976433    0.9738    0.972633\n",
       "           3        6.158849   0.976867    0.9750    0.973833\n",
       "           4        6.976142   0.977167    0.9749    0.973567\n",
       "           5        5.810796   0.980533    0.9800    0.977167\n",
       "60         1        4.685288   0.961583    0.9603    0.958556\n",
       "           2        4.430913   0.972208    0.9709    0.970472\n",
       "           3        4.408038   0.975458    0.9774    0.972056\n",
       "           4        5.132394   0.975625    0.9744    0.972333\n",
       "           5        4.556553   0.981625    0.9819    0.977000\n",
       "70         1        3.403507   0.954833    0.9528    0.949833\n",
       "           2        4.099883   0.971667    0.9708    0.966238\n",
       "           3        3.997659   0.969667    0.9664    0.965929\n",
       "           4        4.052327   0.976667    0.9716    0.970381\n",
       "           5        4.111852   0.976222    0.9740    0.970929\n",
       "80         1        2.630246   0.945417    0.9445    0.934937\n",
       "           2        2.441411   0.968417    0.9646    0.958229\n",
       "           3        2.241837   0.971750    0.9648    0.961542\n",
       "           4        2.417673   0.976417    0.9705    0.968333\n",
       "           5        2.273694   0.981417    0.9745    0.970792\n",
       "90         1        1.100159   0.927833    0.9118    0.899648\n",
       "           2        1.134328   0.945833    0.9259    0.920204\n",
       "           3        1.138127   0.962667    0.9441    0.939704\n",
       "           4        1.115206   0.975167    0.9613    0.957167\n",
       "           5        1.122202   0.977833    0.9610    0.956981\n",
       "99         1        0.108739   0.245066    0.1919    0.187012\n",
       "           2        0.105728   0.697368    0.6024    0.597074\n",
       "           3        0.123367   0.858553    0.7562    0.746785\n",
       "           4        0.106161   0.888158    0.7833    0.779407\n",
       "           5        0.161261   0.896382    0.7764    0.765570"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.concat(results).set_index(['percentage', 'epoch'])\n",
    "\n",
    "results.to_csv('results/SISA.csv')\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05ab70c-88b3-46ee-9763-4b7e51eea1a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
